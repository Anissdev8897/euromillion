#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyseur avanc√© de tirages d'Euromillions
Ce script analyse les statistiques des tirages d'Euromillions et propose des combinaisons
en utilisant des techniques avanc√©es de pr√©diction.

Am√©liorations:
- Correction de l'affichage des valeurs np.int64
- Meilleure gestion des erreurs
- Optimisation du code
- Am√©lioration de la documentation
- Standardisation du format de pr√©sentation des combinaisons
- Int√©gration am√©lior√©e de la pond√©ration Fibonacci invers√©e avec poids configurable.
- Optimisation des hyperparam√®tres des mod√®les de Machine Learning (Gradient Boosting, RandomForest).
- √âvaluation enrichie des mod√®les ML avec F1-score, pr√©cision et rappel.
- Correction des avertissements Pylance concernant les variables non d√©finies.
- Correction de l'initialisation du logger pour √©viter les TypeError.
- Correction de l'erreur `ValueError: y should be a 1d array` lors de l'optimisation ML
  en adaptant `RandomizedSearchCV` pour travailler directement avec `OneVsRestClassifier`.
"""

import os
import sys
import argparse
import logging # Importation de logging en premier
import warnings
import random # Ajouter pour Monte Carlo
import platform  # Pour d√©tecter le syst√®me d'exploitation
from itertools import combinations as iter_combinations # Pour les syst√®mes r√©ducteurs
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Set, Counter as CounterType, Optional, Union, Any
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from scipy import stats
# ‚ö†Ô∏è CRITIQUE : Configurer joblib AVANT l'import pour √©viter l'erreur _winapi.CreateProcess
# Cette erreur se produit avec le backend 'loky' (par d√©faut) qui essaie de compter les c≈ìurs CPU
if platform.system() == 'Windows':
    # D√©finir les variables d'environnement AVANT d'importer joblib
    os.environ['JOBLIB_START_METHOD'] = 'threading'
    os.environ['JOBLIB_TEMP_FOLDER'] = os.path.join(os.path.expanduser('~'), '.joblib')
    # D√©sactiver compl√®tement le multiprocessing pour √©viter l'erreur _winapi.CreateProcess
    os.environ['JOBLIB_MULTIPROCESSING'] = '0'
    # Forcer le nombre de c≈ìurs √† 1 pour √©viter le comptage
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
    os.environ['NUMEXPR_NUM_THREADS'] = '1'

import joblib
from joblib import parallel_backend  # Pour forcer le backend threading sur Windows
from sklearn.cluster import KMeans
import traceback

# ‚ö†Ô∏è CRITIQUE : Patcher joblib pour √©viter l'erreur _count_physical_cores sur Windows
if platform.system() == 'Windows':
    try:
        # Patcher la fonction _count_physical_cores pour qu'elle retourne 1 sans essayer de compter
        import joblib.externals.loky.backend.context as loky_context
        original_count_physical_cores = loky_context._count_physical_cores
        
        def patched_count_physical_cores():
            """Version patch√©e qui retourne 1 sans essayer de compter les c≈ìurs."""
            return 1
        
        loky_context._count_physical_cores = patched_count_physical_cores
    except Exception:
        # Si le patch √©choue, continuer (l'erreur peut toujours se produire)
        pass

# Fonction pour obtenir le nombre de jobs de mani√®re s√ªre (√©vite l'erreur joblib sur Windows)
def get_n_jobs():
    """
    Retourne le nombre de jobs √† utiliser pour le parall√©lisme.
    ‚ö†Ô∏è CRITIQUE : Sur Windows, forcer n_jobs=1 pour √©viter l'erreur _winapi.CreateProcess
    Cette erreur se produit avec joblib lors de la cr√©ation de processus en parall√®le.
    """
    # Sur Windows, forcer n_jobs=1 pour √©viter l'erreur joblib
    if platform.system() == 'Windows':
        return 1
    try:
        # Essayer d'obtenir le nombre de c≈ìurs CPU
        n_cores = os.cpu_count()
        if n_cores is None:
            return 1
        # Utiliser au maximum 2 c≈ìurs pour √©viter la surcharge
        return min(2, n_cores)
    except Exception:
        # En cas d'erreur, utiliser 1 seul thread
        return 1

# Configuration du logging (d√©plac√©e en haut)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("EuromillionsAdvancedAnalyzer")


# Importer le module de pond√©ration Fibonacci
# Assurez-vous que fibonacci_weighting.py est accessible dans le PYTHONPATH
try:
    from fibonacci_weighting import apply_inverse_fibonacci_weights
except ImportError:
    # Fallback pour l'environnement de test si le fichier n'est pas directement dans le path
    # Ceci est une solution temporaire si le module n'est pas install√© ou accessible
    # Dans un environnement de production, fibonacci_weighting.py devrait √™tre correctement importable
    logger.warning("Le module 'fibonacci_weighting' n'a pas pu √™tre import√© directement. Tentative d'import via sys.path.")
    sys.path.append(os.path.dirname(__file__)) # Ajoute le r√©pertoire courant au path
    try:
        from fibonacci_weighting import apply_inverse_fibonacci_weights
    except ImportError:
        logger.error("Impossible d'importer le module 'fibonacci_weighting'. La fonctionnalit√© Fibonacci ne sera pas disponible.")
        # D√©finir une fonction de remplacement pour √©viter les erreurs
        def apply_inverse_fibonacci_weights(counts: CounterType[int], reverse_order: bool = True) -> Dict[int, float]:
            return {item: 0.0 for item in counts.keys()}

# Importer l'encodeur avanc√©
try:
    from advanced_encoder import AdvancedEuromillionsEncoder
    ADVANCED_ENCODER_AVAILABLE = True
except ImportError:
    ADVANCED_ENCODER_AVAILABLE = False
    logger.warning("Encodeur avanc√© non disponible. Utilisation des features de base.")

# Import du syst√®me quantique inspir√©
try:
    from quantum_inspired_predictor import QuantumInspiredPredictor, PENNYLANE_AVAILABLE, TORCH_AVAILABLE
    QUANTUM_AVAILABLE = True
    logger.info("‚úÖ Module quantum_inspired_predictor disponible")
except ImportError:
    QUANTUM_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Module quantum_inspired_predictor non disponible - Installation: pip install pennylane torch")

# Ignorer les avertissements
warnings.filterwarnings("ignore")


# Constantes par d√©faut
DEFAULT_CONFIG = {
    "csv_file": "tirage_euromillions.csv",
    "window_draws": 1844, # Nombre de tirages √† consid√©rer pour les statistiques r√©centes
    "num_hot": 8, # Nombre de num√©ros "chauds" √† identifier
    "num_cold": 8, # Nombre de num√©ros "froids" √† identifier
    "propose_size": 5, # Nombre de num√©ros principaux dans une combinaison
    "star_hot": 3, # Nombre d'√©toiles "chaudes" √† identifier
    "star_cold": 3, # Nombre d'√©toiles "froides" √† identifier
    "star_size": 2, # Nombre d'√©toiles dans une combinaison
    "output_dir": "resultats_euromillions_advanced", # R√©pertoire de sortie pour les rapports et graphiques
    "max_number": 50, # Nombre maximum de num√©ros principaux possibles
    "max_star": 12, # Nombre maximum d'√©toiles possibles
    "combinations_to_generate": 10, # Nombre de combinaisons finales √† proposer
    "use_ml": True, # Utiliser les mod√®les de Machine Learning
    "use_temporal": True, # Utiliser l'analyse des tendances temporelles
    "use_correlation": True, # Utiliser l'analyse des corr√©lations
    "use_clustering": True, # Utiliser l'analyse de clustering
    "prediction_weight": 0.7, # Poids des pr√©dictions ML dans le score final
    "test_size": 0.2, # Taille de l'ensemble de test pour l'entra√Ænement ML
    "cv_folds": 5, # Nombre de folds pour la validation crois√©e
    "model_dir": "models", # R√©pertoire pour sauvegarder les mod√®les ML
    "gap_weight": 0.1, # Poids du score d'√©cart dans le score final
    "export_excel": False, # Exporter les r√©sultats vers Excel
    "analyze_parity": True, # Analyser la parit√© des num√©ros
    "analyze_sum": True, # Analyser la somme des num√©ros
    "use_fibonacci_inverse": True,  # Utiliser la pond√©ration Fibonacci invers√©e
    "fibonacci_inverse_weight_blend": 0.3, # Poids pour le m√©lange Fibonacci (0.0 √† 1.0)
    "monte_carlo_simulations": 5000, # Nombre de simulations Monte Carlo
    "max_wheeling_combinations": 50, # Limite le nombre de combinaisons g√©n√©r√©es par le syst√®me r√©ducteur
    "wheeling_num_count": 10, # Nombre de num√©ros √† inclure dans le syst√®me r√©ducteur
    "wheeling_star_count": 5, # Nombre d'√©toiles √† inclure dans le syst√®me r√©ducteur
    "max_combination_history": 100, # Taille maximale de l'historique des combinaisons g√©n√©r√©es
    "max_performance_history": 100, # Taille maximale de l'historique de performance
    "score_weight_prediction": 0.5, # Poids des pr√©dictions ML/fr√©quence dans le score final combin√©
    "score_weight_gap": 0.3, # Poids du score d'√©cart dans le score final combin√©
    "score_weight_frequency": 0.2, # Poids de la fr√©quence r√©cente dans le score final combin√©
}

def _optimize_model_hyperparameters(
    model_name: str,
    estimator: Any, # L'estimateur √† optimiser (maintenant peut √™tre OneVsRestClassifier)
    X_train: np.ndarray,
    y_train_multi: np.ndarray,
    param_distributions: Dict[str, List[Any]],
    n_iter_search: int = 20, # Nombre d'it√©rations pour RandomizedSearchCV
    cv_folds: int = 3, # Nombre de folds pour la validation crois√©e
    random_state: int = 42
) -> Any:
    """
    Optimise les hyperparam√®tres d'un mod√®le en utilisant RandomizedSearchCV.

    Args:
        model_name (str): Nom du mod√®le (pour le logging).
        estimator (Any): L'instance du classifieur √† optimiser (peut √™tre OneVsRestClassifier).
        X_train (np.ndarray): Les donn√©es d'entra√Ænement (features).
        y_train_multi (np.ndarray): Les cibles d'entra√Ænement (format multi-label).
        param_distributions (Dict[str, List[Any]]): Dictionnaire des distributions de param√®tres √† √©chantillonner.
            Les cl√©s doivent √™tre au format 'estimator__param√®tre' si l'estimateur est encapsul√©.
        n_iter_search (int): Nombre d'it√©rations pour RandomizedSearchCV.
        cv_folds (int): Nombre de folds pour la validation crois√©e.
        random_state (int): Graine pour la reproductibilit√©.

    Returns:
        Any: Le meilleur estimateur trouv√© apr√®s optimisation.
    """
    logger.info(f"D√©marrage de l'optimisation des hyperparam√®tres pour {model_name}...")
    
    # ‚ö†Ô∏è CRITIQUE : Sur Windows, forcer n_jobs=1 pour √©viter l'erreur _winapi.CreateProcess
    # M√™me avec get_n_jobs(), RandomizedSearchCV peut causer des probl√®mes sur Windows
    if platform.system() == 'Windows':
        n_jobs = 1  # Forcer n_jobs=1 sur Windows
        logger.debug("Windows d√©tect√© - Utilisation de n_jobs=1 pour RandomizedSearchCV")
    else:
        n_jobs = get_n_jobs()
    
    random_search = RandomizedSearchCV(
        estimator=estimator, # L'estimateur peut √™tre OneVsRestClassifier ici
        param_distributions=param_distributions,
        n_iter=n_iter_search,
        cv=cv_folds,
        scoring='f1_micro', # Utilisation du F1-score micro pour les probl√®mes multi-label
        random_state=random_state,
        n_jobs=n_jobs, # ‚ö†Ô∏è CRITIQUE : n_jobs=1 sur Windows pour √©viter l'erreur joblib
        verbose=1 # Affiche la progression
    )
    
    random_search.fit(X_train, y_train_multi)
    
    logger.info(f"Optimisation termin√©e pour {model_name}.")
    logger.info(f"Meilleurs param√®tres pour {model_name}: {random_search.best_params_}")
    logger.info(f"Meilleur score F1-micro pour {model_name}: {random_search.best_score_:.4f}")
    
    return random_search.best_estimator_


class EuromillionsAdvancedAnalyzer:
    """Classe principale pour l'analyse avanc√©e des tirages d'Euromillions."""
    
    def __init__(self, config: Dict = None):
        """
        Initialise l'analyseur avec la configuration sp√©cifi√©e.
        
        Args:
            config: Dictionnaire de configuration ou None pour utiliser les valeurs par d√©faut
        """
        self.config = DEFAULT_CONFIG.copy()
        if config:
            self.config.update(config)
        
        # üé• NOUVEAU: R√©cup√©rer les embeddings vid√©o depuis la config
        self.video_embeddings = self.config.get("video_embeddings", None)
        if self.video_embeddings:
            logger.info(f"üé• Embeddings vid√©o re√ßus: {len(self.video_embeddings)} vid√©os")
        
        self.df = None
        self.number_cols = []
        self.star_cols = []
        self.output_dir = Path(self.config["output_dir"])
        self.model_dir = Path(self.config["model_dir"])
        
        # Cr√©ation des r√©pertoires n√©cessaires
        for directory in [self.output_dir, self.model_dir]:
            if not directory.exists():
                directory.mkdir(parents=True)
                logger.info(f"R√©pertoire cr√©√©: {directory}")
        
        # Initialisation des mod√®les (MLP par d√©faut, d'autres peuvent √™tre ajout√©s)
        self.number_model = None # Sera initialis√© comme OneVsRestClassifier(GradientBoostingClassifier)
        self.star_model = None   # Sera initialis√© comme OneVsRestClassifier(GradientBoostingClassifier)
        self.rf_number_model = None # Mod√®le RandomForest pour num√©ros
        self.rf_star_model = None   # Mod√®le RandomForest pour √©toiles
        self.scaler_numbers = StandardScaler()
        self.scaler_stars = StandardScaler()  # Scaler s√©par√© pour les √©toiles
        
        # Stockage des r√©sultats d'analyse
        self.freq = None # Fr√©quence globale des num√©ros
        self.var_monthly = None # Variance mensuelle des num√©ros
        self.star_freq = None # Fr√©quence globale des √©toiles
        self.star_var_monthly = None # Variance mensuelle des √©toiles
        
        # ‚ö†Ô∏è CRITIQUE : Charger les scalers sauvegard√©s si disponibles (apr√®s initialisation de toutes les variables)
        self._load_saved_scalers()
    
    def _load_saved_scalers(self) -> None:
        """
        Charge les scalers sauvegard√©s depuis les fichiers joblib si disponibles.
        ‚ö†Ô∏è CRITIQUE : Cette m√©thode doit √™tre appel√©e pour √©viter l'erreur "not fitted yet".
        """
        try:
            scaler_numbers_path = self.model_dir / "scaler_numbers.joblib"
            if scaler_numbers_path.exists():
                try:
                    self.scaler_numbers = joblib.load(scaler_numbers_path)
                    logger.info(f"‚úÖ Scaler num√©ros charg√© depuis {scaler_numbers_path}")
                except Exception as e:
                    logger.warning(f"Impossible de charger le scaler num√©ros: {str(e)}. Cr√©ation d'un nouveau scaler.")
                    self.scaler_numbers = StandardScaler()
            
            scaler_stars_path = self.model_dir / "scaler_stars.joblib"
            if scaler_stars_path.exists():
                try:
                    self.scaler_stars = joblib.load(scaler_stars_path)
                    logger.info(f"‚úÖ Scaler √©toiles charg√© depuis {scaler_stars_path}")
                except Exception as e:
                    logger.warning(f"Impossible de charger le scaler √©toiles: {str(e)}. Cr√©ation d'un nouveau scaler.")
                    self.scaler_stars = StandardScaler()
        except Exception as e:
            logger.warning(f"Erreur lors du chargement des scalers: {str(e)}")
            # Continuer avec des scalers vides
        self.window_counts = None # Fr√©quence des num√©ros sur la fen√™tre r√©cente
        self.star_window_counts = None # Fr√©quence des √©toiles sur la fen√™tre r√©cente
        self.hot = None # Num√©ros "chauds"
        self.cold = None # Num√©ros "froids"
        self.star_hot = None # √âtoiles "chaudes"
        self.star_cold = None # √âtoiles "froides"
        self.number_correlations = None # Corr√©lations entre num√©ros
        self.star_correlations = None # Corr√©lations entre √©toiles
        self.number_clusters = None # Statistiques des clusters de num√©ros
        self.star_clusters = None # Statistiques des clusters d'√©toiles
        self.temporal_patterns = None # Tendances temporelles des num√©ros
        self.star_temporal_patterns = None # Tendances temporelles des √©toiles
        self.number_predictions = None # Probabilit√©s/scores pr√©dits pour les num√©ros
        self.star_predictions = None # Probabilit√©s/scores pr√©dits pour les √©toiles
        self.number_gap_scores = None # Scores d'√©cart pour les num√©ros
        self.star_gap_scores = None # Scores d'√©cart pour les √©toiles
        
        # Nouvelles variables pour les analyses suppl√©mentaires
        self.parity_stats = None # Statistiques de parit√©
        self.sum_stats = None # Statistiques de somme
        self.sum_ranges = None # Distribution des sommes par plage
        self.most_common_sums = None # Sommes les plus fr√©quentes
        self.sequence_stats = None # Statistiques des s√©quences de num√©ros
        
        # Variables pour le suivi des performances
        self.generated_combinations_history = [] # Liste de tuples (date, [combinaisons])
        self.performance_history = [] # Liste de dictionnaires {date: ..., metrics: ...}
        
        # Initialiser l'encodeur avanc√© si disponible
        self.advanced_encoder = None
        if ADVANCED_ENCODER_AVAILABLE:
            try:
                enable_ai_reflection = self.config.get('enable_ai_reflection', True)
                llm_config = self.config.get('llm_config', 'openai')
                self.advanced_encoder = AdvancedEuromillionsEncoder(
                    enable_ai_reflection=enable_ai_reflection,
                    llm_config=llm_config
                )
                logger.info("Encodeur avanc√© initialis√© - Am√©lioration de la pr√©cision activ√©e")
            except Exception as e:
                logger.warning(f"Erreur lors de l'initialisation de l'encodeur avanc√©: {str(e)}")
                self.advanced_encoder = None
        
        # Initialiser le pr√©dicteur quantique si disponible
        # ‚ö†Ô∏è CRITIQUE : Activer le syst√®me quantique par d√©faut pour tous les entra√Ænements et pr√©dictions
        self.quantum_predictor = None
        use_quantum = self.config.get('use_quantum', True)  # Activ√© par d√©faut
        if QUANTUM_AVAILABLE and use_quantum:
            try:
                quantum_config = {
                    'max_number': self.config['max_number'],
                    'max_star': self.config['max_star'],
                    'n_numbers': self.config['propose_size'],
                    'n_stars': self.config['star_size'],
                    'use_qnn': self.config.get('use_qnn', True),
                    'use_qlstm': self.config.get('use_qlstm', True),
                    'use_quantum_annealing': self.config.get('use_quantum_annealing', True),
                }
                self.quantum_predictor = QuantumInspiredPredictor(quantum_config)
                self.config['use_quantum'] = True  # Activer dans la config
                logger.info("‚úÖ Pr√©dicteur quantique initialis√© - Syst√®me Quantum-Inspired activ√©")
            except Exception as e:
                logger.warning(f"Erreur lors de l'initialisation du pr√©dicteur quantique: {str(e)}")
                logger.debug(traceback.format_exc())
                self.quantum_predictor = None
                self.config['use_quantum'] = False
    
    def _convert_to_int_list(self, number_list):
        """
        Convertit une liste de nombres (potentiellement des np.int64) en liste d'entiers Python standard.
        
        Args:
            number_list: Liste de nombres √† convertir
            
        Returns:
            Liste d'entiers Python standard
        """
        if number_list is None:
            return []
        return [int(num) for num in number_list]
    
    def load_data(self) -> bool:
        """
        Charge les donn√©es du fichier CSV.
        Utilise le fichier de cycles s'il existe (tirage_euromillions_complet_cycles.csv).
        Respecte l'ordre chronologique du premier au dernier tirage.
        
        Returns:
            bool: True si le chargement a r√©ussi, False sinon
        """
        csv_path = Path(self.config["csv_file"])
        
        # ‚ö†Ô∏è CRITIQUE : V√©rifier si le fichier de cycles existe
        cycle_file = csv_path.parent / f"{csv_path.stem}_cycles.csv"
        use_cycle_file = False
        
        if cycle_file.exists():
            logger.info(f"Fichier de cycles trouv√©: {cycle_file}")
            logger.info("V√©rification du contenu du fichier de cycles...")
            
            try:
                # V√©rifier que le fichier cycle a les colonnes n√©cessaires
                cycle_df_test = pd.read_csv(cycle_file, nrows=1)
                required_cols = ['Date', 'N1', 'N2', 'N3', 'N4', 'N5', 'E1', 'E2']
                missing_cols = [col for col in required_cols if col not in cycle_df_test.columns]
                
                if not missing_cols:
                    # V√©rifier que la colonne Date existe et n'est pas vide
                    cycle_df_full = pd.read_csv(cycle_file)
                    if 'Date' in cycle_df_full.columns and not cycle_df_full['Date'].isna().all():
                        use_cycle_file = True
                        logger.info("‚úÖ Fichier de cycles valide avec dates - Utilisation du fichier de cycles")
                    else:
                        logger.warning("‚ö†Ô∏è Fichier de cycles sans dates valides - Utilisation du fichier principal")
                else:
                    logger.warning(f"‚ö†Ô∏è Fichier de cycles incomplet (colonnes manquantes: {missing_cols}) - Utilisation du fichier principal")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lors de la v√©rification du fichier de cycles: {str(e)} - Utilisation du fichier principal")
        
        try:
            # Utiliser le fichier de cycles si disponible et valide
            if use_cycle_file:
                logger.info(f"Chargement des donn√©es depuis le fichier de cycles: {cycle_file}")
                self.df = pd.read_csv(cycle_file)
                
                # ‚ö†Ô∏è CRITIQUE : V√©rifier et convertir la colonne Date
                if 'Date' in self.df.columns:
                    self.df['Date'] = pd.to_datetime(self.df['Date'], errors='coerce')
                    # V√©rifier qu'il n'y a pas de dates manquantes
                    if self.df['Date'].isna().any():
                        logger.warning(f"‚ö†Ô∏è {self.df['Date'].isna().sum()} dates manquantes dans le fichier de cycles")
                
                # ‚ö†Ô∏è CRITIQUE : Trier par date du premier au dernier tirage (ordre chronologique)
                if 'Date' in self.df.columns:
                    self.df = self.df.sort_values('Date', ascending=True).reset_index(drop=True)
                    logger.info(f"‚úÖ Donn√©es tri√©es par date (ordre chronologique: {self.df['Date'].min()} ‚Üí {self.df['Date'].max()})")
                else:
                    # Trier par Index si pas de Date
                    if 'Index' in self.df.columns:
                        self.df = self.df.sort_values('Index', ascending=True).reset_index(drop=True)
                        logger.info("‚úÖ Donn√©es tri√©es par Index (ordre chronologique)")
            else:
                # Utiliser le fichier principal
                if not csv_path.exists():
                    logger.error(f"Fichier CSV introuvable: {csv_path}")
                    return False
                    
                logger.info(f"Chargement des donn√©es depuis {csv_path}")
                self.df = pd.read_csv(csv_path)
                
                # ‚ö†Ô∏è CRITIQUE : V√©rifier et cr√©er la colonne Date si manquante
                if 'Date' not in self.df.columns:
                    logger.warning("Colonne 'Date' non trouv√©e. Cr√©ation de dates automatiques...")
                    from datetime import datetime, timedelta
                    first_draw_date = datetime(2004, 2, 13)
                    for i in range(len(self.df)):
                        weeks = i // 2
                        day_in_week = (i % 2) * 3  # 0 pour mardi, 3 pour vendredi
                        date = first_draw_date + timedelta(weeks=weeks, days=day_in_week)
                        self.df.loc[i, 'Date'] = date
                    logger.info("‚úÖ Dates automatiques cr√©√©es")
                
                # Convertir la colonne Date en datetime
                if 'Date' in self.df.columns:
                    self.df['Date'] = pd.to_datetime(self.df['Date'], errors='coerce')
                
                # ‚ö†Ô∏è CRITIQUE : Trier par date du premier au dernier tirage (ordre chronologique)
                if 'Date' in self.df.columns and not self.df['Date'].isna().all():
                    self.df = self.df.sort_values('Date', ascending=True).reset_index(drop=True)
                    logger.info(f"‚úÖ Donn√©es tri√©es par date (ordre chronologique: {self.df['Date'].min()} ‚Üí {self.df['Date'].max()})")
            
            # Appliquer l'encodeur avanc√© si disponible pour am√©liorer les features
            if self.advanced_encoder is not None:
                logger.info("Application de l'encodeur avanc√© pour am√©liorer les features...")
                try:
                    # Encoder toutes les features avanc√©es (incluant les vid√©os si disponibles)
                    self.df = self.advanced_encoder.encode_features(self.df, video_embeddings=self.video_embeddings)
                    logger.info("Features avanc√©es encod√©es avec succ√®s")
                except Exception as e:
                    logger.warning(f"Erreur lors de l'encodage avanc√© (utilisation des features de base): {str(e)}")
            
            # Identification des colonnes de num√©ros et d'√©toiles
            # Pour Euromillions, on s'attend √† des colonnes N1-N5 et E1-E2
            self.number_cols = [col for col in self.df.columns if col.startswith('N')]
            self.star_cols = [col for col in self.df.columns if col.startswith('E')]
            
            if not self.number_cols or len(self.number_cols) != 5:
                logger.error(f"Format de colonnes incorrect pour les num√©ros principaux. Attendu: 5 colonnes commen√ßant par 'N', trouv√©: {len(self.number_cols)}")
                return False
                
            if not self.star_cols or len(self.star_cols) != 2:
                logger.error(f"Format de colonnes incorrect pour les √©toiles. Attendu: 2 colonnes commen√ßant par 'E', trouv√©: {len(self.star_cols)}")
                return False
            
            # V√©rification des types de donn√©es
            for col in self.number_cols + self.star_cols:
                if not pd.api.types.is_numeric_dtype(self.df[col]):
                    try:
                        self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
                        logger.warning(f"Colonne {col} convertie en type num√©rique")
                    except:
                        logger.error(f"Impossible de convertir la colonne {col} en type num√©rique")
                        return False
            
            # Suppression des lignes avec des valeurs manquantes
            initial_rows = len(self.df)
            self.df = self.df.dropna(subset=self.number_cols + self.star_cols)
            if len(self.df) < initial_rows:
                logger.warning(f"{initial_rows - len(self.df)} lignes supprim√©es car contenant des valeurs manquantes")
            
            # Ajout d'une colonne de date si elle n'existe pas
            if 'Date' not in self.df.columns:
                logger.warning("Colonne 'Date' non trouv√©e, cr√©ation d'une colonne de date fictive")
                # Cr√©er une s√©rie de dates en partant de la plus r√©cente (aujourd'hui)
                end_date = datetime.now()
                # Supposer un tirage tous les 3-4 jours (mardi et vendredi pour Euromillions)
                dates = [(end_date - timedelta(days=i*3.5)).strftime('%Y-%m-%d') for i in range(len(self.df))]
                dates.reverse()  # Pour avoir les dates dans l'ordre chronologique
                self.df['Date'] = dates
            
            # Conversion de la date
            try:
                self.df['Date'] = pd.to_datetime(self.df['Date'], errors='coerce')
                # Tri par date
                self.df = self.df.sort_values('Date')
            except Exception as e:
                logger.warning(f"Erreur lors de la conversion des dates: {str(e)}")
            
            logger.info(f"Donn√©es charg√©es avec succ√®s: {len(self.df)} tirages, {len(self.number_cols)} num√©ros principaux et {len(self.star_cols)} √©toiles par tirage")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement des donn√©es: {str(e)}")
            logger.debug(traceback.format_exc())
            return False
    
    def compute_global_stats(self) -> None:
        """
        Calcule les statistiques globales sur tous les tirages.
        """
        logger.info("Calcul des statistiques globales...")
        
        try:
            # Statistiques des num√©ros principaux
            all_numbers = self.df[self.number_cols].values.flatten()
            all_numbers = all_numbers[~pd.isna(all_numbers)].astype(int)
            self.freq = Counter(all_numbers)
            
            # Statistiques des √©toiles
            all_stars = self.df[self.star_cols].values.flatten()
            all_stars = all_stars[~pd.isna(all_stars)].astype(int)
            self.star_freq = Counter(all_stars)
            
            # Calcul des variances (si date disponible)
            self.var_monthly = {}
            self.star_var_monthly = {}
            
            if 'Date' in self.df.columns:
                try:
                    # Variance des num√©ros principaux
                    df_long = self.df.melt(id_vars=["Date"], value_vars=self.number_cols, value_name="number").dropna()
                    df_long["number"] = df_long["number"].astype(int)
                    df_long["month"] = df_long["Date"].dt.to_period("M")
                    monthly = df_long.groupby(["month", "number"]).size().unstack(fill_value=0)
                    self.var_monthly = monthly.var(axis=0).to_dict()
                    
                    # Variance des √©toiles
                    df_long_stars = self.df.melt(id_vars=["Date"], value_vars=self.star_cols, value_name="star").dropna()
                    df_long_stars["star"] = df_long_stars["star"].astype(int)
                    df_long_stars["month"] = df_long_stars["Date"].dt.to_period("M")
                    monthly_stars = df_long_stars.groupby(["month", "star"]).size().unstack(fill_value=0)
                    self.star_var_monthly = monthly_stars.var(axis=0).to_dict()
                except Exception as e:
                    logger.warning(f"Impossible de calculer les variances mensuelles: {str(e)}")
                    logger.debug(traceback.format_exc())
        except Exception as e:
            logger.error(f"Erreur lors du calcul des statistiques globales: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def compute_window_stats(self) -> None:
        """
        Calcule les statistiques sur les derniers tirages (fen√™tre).
        """
        window = self.config["window_draws"]
        logger.info(f"Calcul des statistiques sur les {window} derniers tirages...")
        
        try:
            if len(self.df) < window:
                window = len(self.df)
                logger.warning(f"Nombre de tirages disponibles ({window}) inf√©rieur √† la fen√™tre demand√©e")
            
            recent = self.df.tail(window)
            
            # Statistiques des num√©ros principaux
            nums = recent[self.number_cols].values.flatten()
            nums = nums[~pd.isna(nums)].astype(int)
            self.window_counts = Counter(nums)
            
            # Statistiques des √©toiles
            stars = recent[self.star_cols].values.flatten()
            stars = stars[~pd.isna(stars)].astype(int)
            self.star_window_counts = Counter(stars)
        except Exception as e:
            logger.error(f"Erreur lors du calcul des statistiques de fen√™tre: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def identify_hot_cold(self) -> None:
        """
        Identifie les nombres chauds et froids pour les num√©ros principaux et les √©toiles.
        """
        logger.info("Identification des num√©ros chauds et froids...")
        
        try:
            # Num√©ros principaux
            num_hot = self.config["num_hot"]
            num_cold = self.config["num_cold"]
            unique_nums = len(self.window_counts)
            
            if unique_nums < num_hot + num_cold:
                logger.warning(f"Seulement {unique_nums} num√©ros principaux uniques trouv√©s, ajustement des param√®tres hot/cold")
                if unique_nums <= num_hot:
                    num_hot = max(1, unique_nums - 1)
                    num_cold = 1
                else:
                    num_cold = unique_nums - num_hot
            
            self.hot = [n for n, _ in self.window_counts.most_common(num_hot)]
            self.cold = [n for n, _ in self.window_counts.most_common()][-num_cold:]
            
            # √âtoiles
            star_hot = self.config["star_hot"]
            star_cold = self.config["star_cold"]
            unique_stars = len(self.star_window_counts)
            
            if unique_stars < star_hot + star_cold:
                logger.warning(f"Seulement {unique_stars} √©toiles uniques trouv√©es, ajustement des param√®tres hot/cold")
                if unique_stars <= star_hot:
                    star_hot = max(1, unique_stars - 1)
                    star_cold = 1
                else:
                    star_cold = unique_stars - star_hot
            
            self.star_hot = [n for n, _ in self.star_window_counts.most_common(star_hot)]
            self.star_cold = [n for n, _ in self.star_window_counts.most_common()][-star_cold:]
        except Exception as e:
            logger.error(f"Erreur lors de l'identification des num√©ros chauds/froids: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def analyze_correlations(self) -> None:
        """
        Analyse les corr√©lations entre les num√©ros et entre les √©toiles.
        """
        if not self.config["use_correlation"]:
            return
            
        logger.info("Analyse des corr√©lations entre num√©ros...")
        
        try:
            # Pr√©paration des donn√©es pour les num√©ros principaux
            # Cr√©er un DataFrame o√π chaque colonne repr√©sente un num√©ro et chaque ligne un tirage
            # Les valeurs sont 1 si le num√©ro est pr√©sent, 0 sinon
            number_presence_df = pd.DataFrame(0, index=self.df.index, columns=range(1, self.config["max_number"] + 1))
            for col in self.number_cols:
                for num_val in self.df[col].dropna().unique():
                    number_presence_df[int(num_val)] = self.df[col].apply(lambda x: 1 if x == num_val else 0)
            
            # Calcul de la matrice de corr√©lation
            self.number_correlations = number_presence_df.corr()
            
            # M√™me chose pour les √©toiles
            star_presence_df = pd.DataFrame(0, index=self.df.index, columns=range(1, self.config["max_star"] + 1))
            for col in self.star_cols:
                for star_val in self.df[col].dropna().unique():
                    star_presence_df[int(star_val)] = self.df[col].apply(lambda x: 1 if x == star_val else 0)
            
            self.star_correlations = star_presence_df.corr()
            
            logger.info("Analyse des corr√©lations termin√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse des corr√©lations: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def analyze_temporal_patterns(self) -> None:
        """
        Analyse les tendances temporelles dans les tirages.
        """
        if not self.config["use_temporal"] or 'Date' not in self.df.columns:
            return
            
        logger.info("Analyse des tendances temporelles...")
        
        try:
            # Pr√©paration des donn√©es
            temporal_data = {}
            star_temporal_data = {}
            
            # Grouper par mois ou semaine selon la quantit√© de donn√©es
            # Cr√©er une copie pour √©viter SettingWithCopyWarning
            df_temp = self.df.copy()
            if len(df_temp) > 100:
                # Assez de donn√©es pour une analyse mensuelle
                df_temp['period'] = df_temp['Date'].dt.to_period('M')
                period_type = "mensuelle"
            else:
                # Moins de donn√©es, analyse hebdomadaire
                df_temp['period'] = df_temp['Date'].dt.to_period('W')
                period_type = "hebdomadaire"
            
            # Analyse des num√©ros principaux
            for num in range(1, self.config["max_number"] + 1):
                temporal_data[num] = []
                
                for period, group in df_temp.groupby('period'):
                    # Compter combien de fois le num√©ro appara√Æt dans cette p√©riode
                    count = 0
                    for _, row in group.iterrows():
                        numbers = row[self.number_cols].dropna().astype(int).tolist()
                        if num in numbers:
                            count += 1
                    
                    # Normaliser par le nombre de tirages dans la p√©riode
                    freq = count / len(group) if len(group) > 0 else 0
                    temporal_data[num].append((period, freq))
            
            # Analyse des √©toiles
            for star in range(1, self.config["max_star"] + 1):
                star_temporal_data[star] = []
                
                for period, group in df_temp.groupby('period'):
                    count = 0
                    for _, row in group.iterrows():
                        stars = row[self.star_cols].dropna().astype(int).tolist()
                        if star in stars:
                            count += 1
                    
                    freq = count / len(group) if len(group) > 0 else 0
                    star_temporal_data[star].append((period, freq))
            
            # Stocker les r√©sultats
            self.temporal_patterns = temporal_data
            self.star_temporal_patterns = star_temporal_data
            
            logger.info(f"Analyse temporelle {period_type} termin√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse des tendances temporelles: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def analyze_clustering(self) -> None:
        """
        Analyse les clusters de num√©ros et d'√©toiles.
        """
        if not self.config["use_clustering"]:
            return
            
        logger.info("Analyse des clusters de num√©ros...")
        
        # ‚ö†Ô∏è CRITIQUE : Forcer le backend threading sur Windows pour √©viter l'erreur _winapi.CreateProcess
        # Cette erreur se produit avec le backend 'loky' (par d√©faut) qui essaie de compter les c≈ìurs CPU
        if platform.system() == 'Windows':
            try:
                with parallel_backend('threading', n_jobs=1):
                    self._analyze_clustering_internal()
            except Exception as e:
                logger.warning(f"Erreur avec le backend threading, tentative sans contexte: {str(e)}")
                self._analyze_clustering_internal()
        else:
            self._analyze_clustering_internal()
    
    def _analyze_clustering_internal(self) -> None:
        """
        Impl√©mentation interne de l'analyse de clustering.
        """
        try:
            # Pr√©paration des donn√©es pour les num√©ros principaux
            draws_matrix = np.zeros((len(self.df), self.config["max_number"]))
            
            for i, (_, row) in enumerate(self.df.iterrows()):
                numbers = row[self.number_cols].dropna().astype(int).tolist()
                for num in numbers:
                    if 1 <= num <= self.config["max_number"]:
                        draws_matrix[i, num-1] = 1
            
            # Clustering des tirages
            n_clusters = min(8, len(self.df) // 10)  # Nombre de clusters adaptatif
            if n_clusters < 2:
                n_clusters = 2
            
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_init=1 pour √©viter l'erreur joblib
            # n_init=1 d√©sactive l'optimisation multi-initialisation qui peut d√©clencher le parall√©lisme
            n_init_value = 1 if platform.system() == 'Windows' else 10
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=n_init_value, algorithm='lloyd')
            clusters = kmeans.fit_predict(draws_matrix)
            
            # Analyser les clusters
            cluster_stats = {}
            for cluster_id in range(n_clusters):
                cluster_draws = draws_matrix[clusters == cluster_id]
                cluster_sum = cluster_draws.sum(axis=0)
                cluster_freq = cluster_sum / len(cluster_draws) if len(cluster_draws) > 0 else np.zeros(self.config["max_number"])
                
                # Identifier les num√©ros les plus fr√©quents dans ce cluster
                top_indices = np.argsort(cluster_freq)[-10:]  # Top 10 num√©ros
                top_numbers = [(idx + 1, cluster_freq[idx]) for idx in top_indices]
                
                cluster_stats[cluster_id] = {
                    'size': int(np.sum(clusters == cluster_id)),
                    'top_numbers': top_numbers,
                    'avg_frequency': float(np.mean(cluster_freq))
                }
            
            self.number_clusters = cluster_stats
            
            # M√™me chose pour les √©toiles
            star_matrix = np.zeros((len(self.df), self.config["max_star"]))
            
            for i, (_, row) in enumerate(self.df.iterrows()):
                stars = row[self.star_cols].dropna().astype(int).tolist()
                for star in stars:
                    if 1 <= star <= self.config["max_star"]:
                        star_matrix[i, star-1] = 1
            
            # Moins de clusters pour les √©toiles car moins de combinaisons possibles
            n_star_clusters = min(4, len(self.df) // 20)
            if n_star_clusters < 2:
                n_star_clusters = 2
            
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_init=1 pour √©viter l'erreur joblib
            # n_init=1 d√©sactive l'optimisation multi-initialisation qui peut d√©clencher le parall√©lisme
            n_init_value = 1 if platform.system() == 'Windows' else 10
            kmeans_stars = KMeans(n_clusters=n_star_clusters, random_state=42, n_init=n_init_value, algorithm='lloyd')
            star_clusters = kmeans_stars.fit_predict(star_matrix)
            
            # Analyser les clusters d'√©toiles
            star_cluster_stats = {}
            for cluster_id in range(n_star_clusters):
                cluster_draws = star_matrix[star_clusters == cluster_id]
                cluster_sum = cluster_draws.sum(axis=0)
                cluster_freq = cluster_sum / len(cluster_draws) if len(cluster_draws) > 0 else np.zeros(self.config["max_star"])
                
                top_indices = np.argsort(cluster_freq)[-5:]  # Top 5 √©toiles
                top_stars = [(idx + 1, cluster_freq[idx]) for idx in top_indices]
                
                star_cluster_stats[cluster_id] = {
                    'size': int(np.sum(star_clusters == cluster_id)),
                    'top_stars': top_stars,
                    'avg_frequency': float(np.mean(cluster_freq))
                }
            
            self.star_clusters = star_cluster_stats
            
            logger.info(f"Analyse de clustering termin√©e: {n_clusters} clusters de num√©ros, {n_star_clusters} clusters d'√©toiles")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse des clusters: {str(e)}")
            logger.debug(traceback.format_exc())

    def perform_clustering(self) -> None:
        """
        Ex√©cute l'analyse de clustering. Alias pour analyze_clustering.
        """
        self.analyze_clustering()
    
    def analyze_parity(self) -> None:
        """
        Analyse la parit√© des num√©ros dans les tirages.
        """
        if not self.config["analyze_parity"]:
            return
            
        logger.info("Analyse de la parit√© des num√©ros...")
        
        try:
            parity_counts = []
            
            for _, row in self.df.iterrows():
                numbers = row[self.number_cols].dropna().astype(int).tolist()
                even_count = sum(1 for num in numbers if num % 2 == 0)
                odd_count = len(numbers) - even_count
                parity_counts.append((even_count, odd_count))
            
            # Calculer les statistiques de parit√©
            parity_distribution = Counter(parity_counts)
            most_common = parity_distribution.most_common()
            
            # Calculer les pourcentages
            total_draws = len(self.df)
            parity_stats = {}
            
            for (even, odd), count in most_common:
                parity_stats[f"{even}E-{odd}O"] = {
                    'count': count,
                    'percentage': (count / total_draws) * 100 if total_draws > 0 else 0
                }
            
            self.parity_stats = parity_stats
            
            logger.info("Analyse de parit√© termin√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse de parit√©: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def analyze_sum(self) -> None:
        """
        Analyse la somme des num√©ros dans les tirages.
        """
        if not self.config["analyze_sum"]:
            return
            
        logger.info("Analyse de la somme des num√©ros...")
        
        try:
            sums = []
            
            for _, row in self.df.iterrows():
                numbers = row[self.number_cols].dropna().astype(int).tolist()
                total_sum = sum(numbers)
                sums.append(total_sum)
            
            # Statistiques de base
            min_sum = min(sums) if sums else 0
            max_sum = max(sums) if sums else 0
            avg_sum = sum(sums) / len(sums) if sums else 0
            
            # Distribution des sommes
            sum_counts = Counter(sums)
            most_common_sums = sum_counts.most_common(10)  # Top 10 sommes les plus fr√©quentes
            
            # D√©finir des plages de sommes
            # Calculer les limites des plages de mani√®re plus robuste
            if sums:
                min_val = min(sums)
                max_val = max(sums)
                # Assurer au moins 5 plages, m√™me si la plage est petite
                num_bins = 5
                bin_edges = np.linspace(min_val, max_val + 1, num_bins + 1) # +1 pour inclure max_val
                
                ranges_list = []
                for i in range(num_bins):
                    start = int(bin_edges[i])
                    end = int(bin_edges[i+1])
                    if i == num_bins - 1: # Pour la derni√®re plage, inclure la fin
                        ranges_list.append((start, end))
                    else:
                        ranges_list.append((start, end -1)) # Exclure la fin pour les autres plages
            else:
                ranges_list = []

            # Compter les tirages dans chaque plage
            range_counts = {f"{int(start)}-{int(end)}": 0 for start, end in ranges_list}
            
            for sum_val in sums:
                for i, (start, end) in enumerate(ranges_list):
                    # Ajuster la condition pour la derni√®re plage si n√©cessaire
                    if i == len(ranges_list) -1:
                        if start <= sum_val <= end:
                            range_key = f"{int(start)}-{int(end)}"
                            range_counts[range_key] += 1
                            break
                    else:
                        if start <= sum_val < end:
                            range_key = f"{int(start)}-{int(end)}"
                            range_counts[range_key] += 1
                            break
            
            # Calculer les pourcentages
            total_draws = len(sums)
            for range_key in range_counts:
                count = range_counts[range_key]
                range_counts[range_key] = {
                    'count': count,
                    'percentage': (count / total_draws) * 100 if total_draws > 0 else 0
                }
            
            # Stocker les r√©sultats
            self.sum_stats = {
                'min': int(min_sum),
                'max': int(max_sum),
                'avg': float(avg_sum),
                'median': float(np.median(sums)) if sums else 0,
                'std': float(np.std(sums)) if sums else 0
            }
            
            self.sum_ranges = range_counts
            self.most_common_sums = [(int(sum_val), count) for sum_val, count in most_common_sums]
            
            logger.info("Analyse de somme termin√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse de somme: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def analyze_sequences(self) -> None:
        """
        Analyse les s√©quences de num√©ros cons√©cutifs dans les tirages.
        """
        logger.info("Analyse des s√©quences de num√©ros cons√©cutifs...")
        
        try:
            sequence_counts = []
            
            for _, row in self.df.iterrows():
                numbers = sorted(row[self.number_cols].dropna().astype(int).tolist())
                
                # Compter les s√©quences de num√©ros cons√©cutifs
                sequences = 0
                seq_length = 1
                
                for i in range(1, len(numbers)):
                    if numbers[i] == numbers[i-1] + 1:
                        seq_length += 1
                    else:
                        if seq_length > 1:
                            sequences += 1
                        seq_length = 1
                
                # V√©rifier la derni√®re s√©quence
                if seq_length > 1:
                    sequences += 1
                
                sequence_counts.append(sequences)
            
            # Calculer les statistiques
            sequence_distribution = Counter(sequence_counts)
            
            # Calculer les pourcentages
            total_draws = len(self.df)
            sequence_stats = {}
            
            for seq_count, draw_count in sequence_distribution.items():
                sequence_stats[seq_count] = {
                    'count': draw_count,
                    'percentage': (draw_count / total_draws) * 100 if total_draws > 0 else 0
                }
            
            self.sequence_stats = sequence_stats
            
            logger.info("Analyse des s√©quences termin√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse des s√©quences: {str(e)}")
            logger.debug(traceback.format_exc())
    
    def train_ml_models(self) -> bool:
        """
        Entra√Æne des mod√®les de machine learning pour pr√©dire les num√©ros et les √©toiles.
        
        Returns:
            bool: True si l'entra√Ænement a r√©ussi, False sinon
        """
        if not self.config["use_ml"]:
            return False
            
        logger.info("Entra√Ænement des mod√®les de pr√©diction...")
        
        try:
            # V√©rifier qu'il y a suffisamment de donn√©es
            if len(self.df) < 50:
                logger.warning("Donn√©es insuffisantes pour l'entra√Ænement des mod√®les ML (minimum 50 tirages)")
                return False
            
            # Pr√©paration des donn√©es pour les num√©ros principaux
            # Utiliser l'encodeur avanc√© si disponible
            window_size = 5  # D√©finir window_size au d√©but pour toutes les branches
            y_stars = None  # Initialiser y_stars √† None pour pouvoir v√©rifier s'il a √©t√© cr√©√©
            
            if self.advanced_encoder is not None:
                try:
                    logger.info("Utilisation de l'encodeur avanc√© pour pr√©parer les features ML...")
                    # ‚ö†Ô∏è CRITIQUE : Pr√©parer les features SANS scaler de l'encodeur
                    # On utilisera le scaler de l'analyseur pour garantir la coh√©rence entre entra√Ænement et pr√©diction
                    # üé• NOUVEAU: Passer les embeddings vid√©o √† l'encodeur
                    X_unscaled, y = self.advanced_encoder.prepare_ml_features(
                        self.df, 
                        use_scaler=False,
                        video_embeddings=self.video_embeddings
                    )
                    
                    # S√©parer les targets pour num√©ros et √©toiles
                    y_numbers = y[:, :5].tolist()  # N1-N5
                    y_stars = y[:, 5:7].tolist()   # E1-E2
                    
                    # ‚ö†Ô∏è CRITIQUE : Utiliser le scaler de l'analyseur (pas celui de l'encodeur)
                    # Cela garantit que les features √† l'entra√Ænement et √† la pr√©diction sont identiques
                    X = self.scaler_numbers.fit_transform(X_unscaled)
                    
                    logger.info(f"Features avanc√©es pr√©par√©es: {X.shape[0]} √©chantillons, {X.shape[1]} features")
                    logger.info(f"Targets pr√©par√©s: {len(y_numbers)} num√©ros, {len(y_stars)} √©toiles")
                except Exception as e:
                    logger.warning(f"Erreur avec l'encodeur avanc√©, utilisation des features de base: {str(e)}")
                    # Continuer avec la m√©thode de base
                    X = []
                    y_numbers = []
                    y_stars = []
                    for i in range(len(self.df) - window_size):
                        features = []
                        for j in range(window_size):
                            row = self.df.iloc[i + j]
                            numbers = row[self.number_cols].dropna().astype(int).tolist()
                            stars = row[self.star_cols].dropna().astype(int).tolist()
                            features.extend(numbers)
                            features.extend(stars)
                        X.append(features)
                        next_row = self.df.iloc[i + window_size]
                        next_numbers = next_row[self.number_cols].dropna().astype(int).tolist()
                        next_stars = next_row[self.star_cols].dropna().astype(int).tolist()
                        y_numbers.append(next_numbers)
                        y_stars.append(next_stars)
                    X = np.array(X)
                    X = self.scaler_numbers.fit_transform(X)
            else:
                # M√©thode de base sans encodeur avanc√©
                X = []
                y_numbers = []
                y_stars = []
                
                # Utiliser une fen√™tre glissante pour cr√©er les features
                for i in range(len(self.df) - window_size):
                    # Features: les window_size derniers tirages
                    features = []
                    
                    for j in range(window_size):
                        row = self.df.iloc[i + j]
                        numbers = row[self.number_cols].dropna().astype(int).tolist()
                        stars = row[self.star_cols].dropna().astype(int).tolist()
                        
                        # Ajouter les num√©ros et √©toiles comme features
                        features.extend(numbers)
                        features.extend(stars)
                    
                    X.append(features)
                    
                    # Target: le tirage suivant
                    next_row = self.df.iloc[i + window_size]
                    next_numbers = next_row[self.number_cols].dropna().astype(int).tolist()
                    next_stars = next_row[self.star_cols].dropna().astype(int).tolist()
                    y_numbers.append(next_numbers)
                    y_stars.append(next_stars)
                
                # Conversion en arrays numpy
                X = np.array(X)
                
                # Normalisation des features
                X = self.scaler_numbers.fit_transform(X)
            
            # S√©paration en ensembles d'entra√Ænement et de test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_numbers, test_size=self.config["test_size"], random_state=42
            )
            
            # Transformation du probl√®me en classification multi-label pour les num√©ros
            y_train_multi = np.zeros((len(y_train), self.config["max_number"]))
            for i, nums in enumerate(y_train):
                for num in nums:
                    if 1 <= num <= self.config["max_number"]:
                        y_train_multi[i, num-1] = 1

            # --- Optimisation et entra√Ænement du mod√®le Gradient Boosting pour les num√©ros ---
            logger.info("Optimisation et entra√Ænement du mod√®le Gradient Boosting pour les num√©ros...")
            gb_params_numbers = {
                'estimator__n_estimators': [50, 100, 150, 200], # Utiliser estimator__ pour cibler l'estimateur de base
                'estimator__learning_rate': [0.01, 0.05, 0.1, 0.2],
                'estimator__max_depth': [3, 4, 5, 6],
                'estimator__subsample': [0.7, 0.8, 0.9, 1.0] 
            }
            # Cr√©er l'estimateur OneVsRestClassifier √† optimiser
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_jobs=1 pour √©viter l'erreur joblib
            if platform.system() == 'Windows':
                n_jobs_ovr = 1
            else:
                n_jobs_ovr = get_n_jobs()
            gb_ovr_estimator_numbers = OneVsRestClassifier(GradientBoostingClassifier(random_state=42), n_jobs=n_jobs_ovr)
            best_gb_numbers_estimator = _optimize_model_hyperparameters(
                "GradientBoostingClassifier (Num√©ros)",
                gb_ovr_estimator_numbers, # Passer le OneVsRestClassifier ici
                X_train, y_train_multi,
                gb_params_numbers,
                n_iter_search=20 
            )
            self.number_model = best_gb_numbers_estimator # best_estimator_ est d√©j√† un OneVsRestClassifier
            # Le .fit est d√©j√† fait par RandomizedSearchCV
            logger.info("Entra√Ænement du mod√®le Gradient Boosting de num√©ros termin√©.")

            # --- Optimisation et entra√Ænement du mod√®le RandomForest pour les num√©ros ---
            logger.info("Optimisation et entra√Ænement du mod√®le RandomForest pour les num√©ros...")
            rf_params_numbers = {
                'n_estimators': [50, 100, 150, 200], 
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10, 20],
                'min_samples_leaf': [1, 2, 4] 
            }
            # RandomForestClassifier g√®re nativement le multi-label, pas besoin de OneVsRestClassifier pour l'optimisation
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_jobs=1 pour √©viter l'erreur joblib
            if platform.system() == 'Windows':
                n_jobs_rf = 1
            else:
                n_jobs_rf = get_n_jobs()
            best_rf_numbers_estimator = _optimize_model_hyperparameters(
                "RandomForestClassifier (Num√©ros)",
                RandomForestClassifier(random_state=42, n_jobs=n_jobs_rf), # Passer le RandomForestClassifier directement
                X_train, y_train_multi,
                rf_params_numbers,
                n_iter_search=20 
            )
            self.rf_number_model = best_rf_numbers_estimator 
            # Le .fit est d√©j√† fait par RandomizedSearchCV
            logger.info("Entra√Ænement du mod√®le RandomForest de num√©ros termin√©.")
            
            # √âvaluation des num√©ros
            logger.info("√âvaluation du mod√®le de num√©ros...")
            # Initialisation explicite pour Pylance
            y_pred_numbers = np.array([]) 
            y_test_multi_eval = np.zeros((len(y_test), self.config["max_number"])) # Renomm√© pour clart√©
            for i, nums in enumerate(y_test):
                for num in nums:
                    if 1 <= num <= self.config["max_number"]:
                        y_test_multi_eval[i, num-1] = 1

            # Utiliser le mod√®le optimis√© pour la pr√©diction
            y_pred_numbers = self.number_model.predict(X_test)
            
            accuracy_numbers = accuracy_score(y_test_multi_eval, y_pred_numbers)
            f1_micro_numbers = f1_score(y_test_multi_eval, y_pred_numbers, average='micro')
            f1_macro_numbers = f1_score(y_test_multi_eval, y_pred_numbers, average='macro')
            precision_micro_numbers = precision_score(y_test_multi_eval, y_pred_numbers, average='micro')
            recall_micro_numbers = recall_score(y_test_multi_eval, y_pred_numbers, average='micro')

            logger.info(f"Pr√©cision (Accuracy) du mod√®le de num√©ros: {accuracy_numbers*100:.2f}%")
            logger.info(f"F1-score (Micro) du mod√®le de num√©ros: {f1_micro_numbers:.4f}")
            logger.info(f"F1-score (Macro) du mod√®le de num√©ros: {f1_macro_numbers:.4f}")
            logger.info(f"Pr√©cision (Micro) du mod√®le de num√©ros: {precision_micro_numbers:.4f}")
            logger.info(f"Rappel (Micro) du mod√®le de num√©ros: {recall_micro_numbers:.4f}")
            logger.info("√âvaluation du mod√®le de num√©ros termin√©e.")
            
            # R√©compenser la r√©flexion IA si disponible
            if self.advanced_encoder is not None and self.advanced_encoder.ai_reflection is not None:
                performance_metrics = {
                    'accuracy': accuracy_numbers,
                    'f1_score': f1_micro_numbers,
                    'precision': precision_micro_numbers,
                    'recall': recall_micro_numbers
                }
                self.advanced_encoder.reward_reflection(performance_metrics)
            
            # Pr√©parer y_stars si pas d√©j√† fait (cas m√©thode de base)
            # Si y_stars est None ou vide, le cr√©er depuis les donn√©es
            if y_stars is None or len(y_stars) == 0:
                logger.info("Cr√©ation de y_stars depuis les donn√©es...")
                y_stars = []
                for i in range(len(self.df) - window_size):
                    next_row = self.df.iloc[i + window_size]
                    next_stars = next_row[self.star_cols].dropna().astype(int).tolist()
                    y_stars.append(next_stars)
                logger.info(f"y_stars cr√©√©: {len(y_stars)} √©chantillons")
            
            # S√©paration en ensembles d'entra√Ænement et de test
            # X_train et X_test sont d√©j√† d√©finis et normalis√©s
            # Cr√©er des indices pour la s√©paration train/test qui correspondent √† ceux utilis√©s pour X
            indices = np.arange(len(y_stars))
            indices_train, indices_test = train_test_split(
                indices, 
                test_size=self.config["test_size"], 
                random_state=42
            )
            
            # Utiliser ces indices pour s√©parer y_stars
            y_train_stars = [y_stars[i] for i in indices_train]
            y_test_stars = [y_stars[i] for i in indices_test]
            
            # S'assurer que les scalers sont fitted (ils devraient l'√™tre d√©j√† via fit_transform)
            # Mais v√©rifier pour √©viter l'erreur "not fitted yet"
            if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                logger.warning("Le scaler_numbers n'est pas fitted, le fit maintenant...")
                self.scaler_numbers.fit(X_train)
            
            # Initialiser et fit le scaler_stars si n√©cessaire
            if not hasattr(self, 'scaler_stars'):
                self.scaler_stars = StandardScaler()
            if not hasattr(self.scaler_stars, 'mean_') or self.scaler_stars.mean_ is None:
                logger.info("Fitting du scaler_stars...")
                self.scaler_stars.fit(X_train)
            
            # Transformation en format multi-label pour les √©toiles
            y_train_stars_multi = np.zeros((len(y_train_stars), self.config["max_star"]))
            for i, stars in enumerate(y_train_stars):
                for star in stars:
                    if 1 <= star <= self.config["max_star"]:
                        y_train_stars_multi[i, star-1] = 1

            # --- Optimisation et entra√Ænement du mod√®le Gradient Boosting pour les √©toiles ---
            logger.info("Optimisation et entra√Ænement du mod√®le Gradient Boosting pour les √©toiles...")
            gb_params_stars = {
                'estimator__n_estimators': [50, 100, 150, 200],
                'estimator__learning_rate': [0.01, 0.05, 0.1, 0.2],
                'estimator__max_depth': [2, 3, 4, 5],
                'estimator__subsample': [0.7, 0.8, 0.9, 1.0]
            }
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_jobs=1 pour √©viter l'erreur joblib
            if platform.system() == 'Windows':
                n_jobs_ovr_stars = 1
            else:
                n_jobs_ovr_stars = get_n_jobs()
            gb_ovr_estimator_stars = OneVsRestClassifier(GradientBoostingClassifier(random_state=42), n_jobs=n_jobs_ovr_stars)
            best_gb_stars_estimator = _optimize_model_hyperparameters(
                "GradientBoostingClassifier (√âtoiles)",
                gb_ovr_estimator_stars,
                X_train, y_train_stars_multi,
                gb_params_stars,
                n_iter_search=10
            )
            self.star_model = best_gb_stars_estimator
            logger.info("Entra√Ænement du mod√®le Gradient Boosting d'√©toiles termin√©.")

            # --- Optimisation et entra√Ænement du mod√®le RandomForest pour les √©toiles ---
            logger.info("Optimisation et entra√Ænement du mod√®le RandomForest pour les √©toiles...")
            rf_params_stars = {
                'n_estimators': [50, 100, 150, 200],
                'max_depth': [3, 7, 10, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            # ‚ö†Ô∏è CRITIQUE : Sur Windows, utiliser n_jobs=1 pour √©viter l'erreur joblib
            if platform.system() == 'Windows':
                n_jobs_rf_stars = 1
            else:
                n_jobs_rf_stars = get_n_jobs()
            best_rf_stars_estimator = _optimize_model_hyperparameters(
                "RandomForestClassifier (√âtoiles)",
                RandomForestClassifier(random_state=42, n_jobs=n_jobs_rf_stars),
                X_train, y_train_stars_multi,
                rf_params_stars,
                n_iter_search=10
            )
            self.rf_star_model = best_rf_stars_estimator
            logger.info("Entra√Ænement du mod√®le RandomForest d'√©toiles termin√©.")
            
            # √âvaluation des √©toiles
            logger.info("√âvaluation du mod√®le d'√©toiles...")
            # Initialisation explicite pour Pylance
            y_pred_stars = np.array([]) 
            y_test_stars_multi_eval = np.zeros((len(y_test_stars), self.config["max_star"])) # Renomm√© pour clart√©
            for i, stars in enumerate(y_test_stars):
                for star in stars:
                    if 1 <= star <= self.config["max_star"]:
                        y_test_stars_multi_eval[i, star-1] = 1

            y_pred_stars = self.star_model.predict(X_test)
            
            accuracy_stars = accuracy_score(y_test_stars_multi_eval, y_pred_stars)
            f1_micro_stars = f1_score(y_test_stars_multi_eval, y_pred_stars, average='micro')
            f1_macro_stars = f1_score(y_test_stars_multi_eval, y_pred_stars, average='macro')
            precision_micro_stars = precision_score(y_test_stars_multi_eval, y_pred_stars, average='micro')
            recall_micro_stars = recall_score(y_test_stars_multi_eval, y_pred_stars, average='micro')

            logger.info(f"Pr√©cision (Accuracy) du mod√®le d'√©toiles: {accuracy_stars*100:.2f}%")
            logger.info(f"F1-score (Micro) du mod√®le d'√©toiles: {f1_micro_stars:.4f}")
            logger.info(f"F1-score (Macro) du mod√®le d'√©toiles: {f1_macro_stars:.4f}")
            logger.info(f"Pr√©cision (Micro) du mod√®le d'√©toiles: {precision_micro_stars:.4f}")
            logger.info(f"Rappel (Micro) du mod√®le d'√©toiles: {recall_micro_stars:.4f}")
            logger.info("√âvaluation du mod√®le d'√©toiles termin√©e.")
            
            # R√©compenser la r√©flexion IA pour les √©toiles aussi
            if self.advanced_encoder is not None and self.advanced_encoder.ai_reflection is not None:
                performance_metrics_stars = {
                    'accuracy': accuracy_stars,
                    'f1_score': f1_micro_stars,
                    'precision': precision_micro_stars,
                    'recall': recall_micro_stars
                }
                # Utiliser la moyenne des m√©triques num√©ros et √©toiles pour la r√©compense globale
                combined_metrics = {
                    'accuracy': (accuracy_numbers + accuracy_stars) / 2,
                    'f1_score': (f1_micro_numbers + f1_micro_stars) / 2,
                    'precision': (precision_micro_numbers + precision_micro_stars) / 2,
                    'recall': (recall_micro_numbers + recall_micro_stars) / 2
                }
                # R√©compenser la r√©flexion IA si la m√©thode existe
                if hasattr(self.advanced_encoder, 'reward_reflection'):
                    try:
                        self.advanced_encoder.reward_reflection(combined_metrics)
                    except Exception as e:
                        logger.debug(f"Impossible de r√©compenser la r√©flexion IA: {str(e)}")
            
            # Sauvegarder les mod√®les entra√Æn√©s
            try:
                logger.info("Sauvegarde des mod√®les entra√Æn√©s...")
                
                # Sauvegarder le mod√®le Gradient Boosting pour num√©ros
                if self.number_model is not None:
                    number_model_path = self.model_dir / "number_model_gb.joblib"
                    joblib.dump(self.number_model, number_model_path)
                    logger.info(f"Mod√®le Gradient Boosting num√©ros sauvegard√©: {number_model_path}")
                
                # Sauvegarder le mod√®le RandomForest pour num√©ros
                if self.rf_number_model is not None:
                    rf_number_model_path = self.model_dir / "number_model_rf.joblib"
                    joblib.dump(self.rf_number_model, rf_number_model_path)
                    logger.info(f"Mod√®le RandomForest num√©ros sauvegard√©: {rf_number_model_path}")
                
                # Sauvegarder le mod√®le Gradient Boosting pour √©toiles
                if self.star_model is not None:
                    star_model_path = self.model_dir / "star_model_gb.joblib"
                    joblib.dump(self.star_model, star_model_path)
                    logger.info(f"Mod√®le Gradient Boosting √©toiles sauvegard√©: {star_model_path}")
                
                # Sauvegarder le mod√®le RandomForest pour √©toiles
                if self.rf_star_model is not None:
                    rf_star_model_path = self.model_dir / "star_model_rf.joblib"
                    joblib.dump(self.rf_star_model, rf_star_model_path)
                    logger.info(f"Mod√®le RandomForest √©toiles sauvegard√©: {rf_star_model_path}")
                
                # Sauvegarder les scalers
                if self.scaler_numbers is not None:
                    scaler_path = self.model_dir / "scaler_numbers.joblib"
                    joblib.dump(self.scaler_numbers, scaler_path)
                    logger.info(f"Scaler num√©ros sauvegard√©: {scaler_path}")
                
                if hasattr(self, 'scaler_stars') and self.scaler_stars is not None:
                    scaler_stars_path = self.model_dir / "scaler_stars.joblib"
                    joblib.dump(self.scaler_stars, scaler_stars_path)
                    logger.info(f"Scaler √©toiles sauvegard√©: {scaler_stars_path}")
                
                # Sauvegarder l'encodeur avanc√© si disponible
                if self.advanced_encoder is not None:
                    encoder_path = self.model_dir / "advanced_encoder.joblib"
                    joblib.dump(self.advanced_encoder, encoder_path)
                    logger.info(f"Encodeur avanc√© sauvegard√©: {encoder_path}")
                
                logger.info("Tous les mod√®les ont √©t√© sauvegard√©s avec succ√®s.")
                
            except Exception as e:
                logger.error(f"Erreur lors de la sauvegarde des mod√®les: {str(e)}")
                logger.debug(traceback.format_exc())
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'entra√Ænement des mod√®les ML: {str(e)}")
            logger.debug(traceback.format_exc())
            return False
    
    def predict_numbers(self) -> Tuple[List[int], List[int]]:
        """
        Pr√©dit les num√©ros et les √©toiles pour le prochain tirage.
        
        Returns:
            Tuple contenant:
            - Liste des num√©ros pr√©dits
            - Liste des √©toiles pr√©dites
        """
        logger.info("Pr√©diction des num√©ros pour le prochain tirage...")
        
        try:
            # Initialiser les pr√©dictions
            number_probs = {}
            star_probs = {}
            
            # 1. Utiliser les statistiques de fr√©quence
            for num in range(1, self.config["max_number"] + 1):
                # Utiliser la fr√©quence sur la fen√™tre r√©cente, normalis√©e par le nombre total de num√©ros tir√©s dans la fen√™tre
                freq = self.window_counts.get(num, 0) / (self.config["propose_size"] * min(self.config["window_draws"], len(self.df)))
                number_probs[num] = freq
            
            for star in range(1, self.config["max_star"] + 1):
                # Utiliser la fr√©quence sur la fen√™tre r√©cente, normalis√©e par le nombre total d'√©toiles tir√©es dans la fen√™tre
                freq = self.star_window_counts.get(star, 0) / (self.config["star_size"] * min(self.config["window_draws"], len(self.df)))
                star_probs[star] = freq
            
            # 2. Ajuster avec les tendances temporelles si disponibles
            if self.temporal_patterns and self.config["use_temporal"]:
                for num in range(1, self.config["max_number"] + 1):
                    if num in self.temporal_patterns and self.temporal_patterns[num]:
                        # Utiliser la tendance la plus r√©cente
                        _, recent_freq = self.temporal_patterns[num][-1]
                        # Blend avec un poids de 0.3 pour la tendance temporelle
                        number_probs[num] = number_probs.get(num, 0) * 0.7 + recent_freq * 0.3
            
            if self.star_temporal_patterns and self.config["use_temporal"]:
                for star in range(1, self.config["max_star"] + 1):
                    if star in self.star_temporal_patterns and self.star_temporal_patterns[star]:
                        _, recent_freq = self.star_temporal_patterns[star][-1]
                        star_probs[star] = star_probs.get(star, 0) * 0.7 + recent_freq * 0.3 # Correction: utiliser recent_freq ici
            
            # 3. Utiliser les pr√©dictions ML si disponibles
            if self.number_model and self.star_model and self.rf_number_model and self.rf_star_model and self.config["use_ml"]:
                # ‚ö†Ô∏è CRITIQUE : Utiliser la m√™me m√©thode de pr√©paration des features que lors de l'entra√Ænement
                # Si l'encodeur avanc√© a √©t√© utilis√© √† l'entra√Ænement, l'utiliser aussi √† la pr√©diction
                if self.advanced_encoder is not None:
                    try:
                        # Utiliser prepare_ml_features pour obtenir les m√™mes features qu'√† l'entra√Ænement
                        # On prend les derniers tirages pour cr√©er les features de pr√©diction
                        window_size = 5
                        if len(self.df) < window_size:
                            logger.warning("Pas assez de donn√©es pour pr√©parer les features ML pour la pr√©diction.")
                        else:
                            # Cr√©er un DataFrame temporaire avec les derniers tirages
                            last_draws_df = self.df.iloc[-window_size:].copy()
                            
                            # ‚ö†Ô∏è CRITIQUE : Utiliser prepare_ml_features mais avec le scaler de l'analyseur (pas celui de l'encodeur)
                            # L'encodeur pr√©pare les features, mais on utilise le scaler de l'analyseur qui a √©t√© entra√Æn√©
                            # üé• NOUVEAU: Passer les embeddings vid√©o pour la pr√©diction
                            X_unscaled, _ = self.advanced_encoder.prepare_ml_features(
                                last_draws_df, 
                                use_scaler=False,
                                video_embeddings=self.video_embeddings
                            )
                            
                            # Prendre la derni√®re ligne (la plus r√©cente) pour la pr√©diction
                            if len(X_unscaled) > 0:
                                features_unscaled = X_unscaled[-1:].reshape(1, -1)  # Reshape pour avoir (1, n_features)
                                
                                # Charger les scalers sauvegard√©s si pas encore charg√©s
                                if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                                    logger.warning("Le scaler_numbers n'est pas fitted. Tentative de chargement depuis le fichier sauvegard√©...")
                                    self._load_saved_scalers()
                                
                                # Utiliser le scaler de l'analyseur (celui qui a √©t√© entra√Æn√©)
                                if hasattr(self.scaler_numbers, 'mean_') and self.scaler_numbers.mean_ is not None:
                                    features_scaled_numbers = self.scaler_numbers.transform(features_unscaled)
                                    features_scaled_stars = features_scaled_numbers  # Utiliser les m√™mes features pour les √©toiles
                                    
                                    logger.info(f"‚úÖ Features pr√©par√©es avec l'encodeur avanc√©: {features_scaled_numbers.shape[1]} features (identique √† l'entra√Ænement)")
                                else:
                                    raise ValueError("Scaler de l'analyseur non disponible")
                            else:
                                raise ValueError("Aucune feature g√©n√©r√©e par l'encodeur avanc√©")
                    except Exception as e:
                        logger.warning(f"Erreur lors de la pr√©paration des features avec l'encodeur avanc√©: {str(e)}")
                        logger.warning("Tentative avec la m√©thode de base...")
                        # Fallback vers la m√©thode de base
                        window_size = 5
                        if len(self.df) < window_size:
                            logger.warning("Pas assez de donn√©es pour pr√©parer les features ML pour la pr√©diction.")
                        else:
                            features_list = []
                            for i in range(1, window_size + 1):
                                row = self.df.iloc[-i] 
                                numbers = row[self.number_cols].dropna().astype(int).tolist()
                                stars = row[self.star_cols].dropna().astype(int).tolist()
                                features_list.extend(numbers)
                                features_list.extend(stars)
                            
                            features_list.reverse()
                            expected_feature_length = window_size * (len(self.number_cols) + len(self.star_cols))
                            while len(features_list) < expected_feature_length:
                                features_list.append(0)
                            
                            features = np.array([features_list[:expected_feature_length]])
                            
                            # Charger les scalers sauvegard√©s si pas encore charg√©s
                            if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                                logger.warning("Le scaler_numbers n'est pas fitted. Tentative de chargement depuis le fichier sauvegard√©...")
                                self._load_saved_scalers()
                            
                            if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                                logger.error("Le scaler_numbers n'est pas fitted et n'a pas pu √™tre charg√©. Impossible de faire des pr√©dictions ML.")
                                logger.warning("Utilisation uniquement des statistiques de fr√©quence pour la pr√©diction.")
                                features_scaled_numbers = None
                                features_scaled_stars = None
                            else:
                                features_scaled_numbers = self.scaler_numbers.transform(features)
                                if hasattr(self, 'scaler_stars') and hasattr(self.scaler_stars, 'mean_') and self.scaler_stars.mean_ is not None:
                                    features_scaled_stars = self.scaler_stars.transform(features)
                                else:
                                    features_scaled_stars = features_scaled_numbers
                else:
                    # M√©thode de base sans encodeur avanc√©
                    window_size = 5
                    if len(self.df) < window_size:
                        logger.warning("Pas assez de donn√©es pour pr√©parer les features ML pour la pr√©diction.")
                        features_scaled_numbers = None
                        features_scaled_stars = None
                    else:
                        features_list = []
                        for i in range(1, window_size + 1):
                            row = self.df.iloc[-i] 
                            numbers = row[self.number_cols].dropna().astype(int).tolist()
                            stars = row[self.star_cols].dropna().astype(int).tolist()
                            features_list.extend(numbers)
                            features_list.extend(stars)
                        
                        features_list.reverse()
                        expected_feature_length = window_size * (len(self.number_cols) + len(self.star_cols))
                        while len(features_list) < expected_feature_length:
                            features_list.append(0)
                        
                        features = np.array([features_list[:expected_feature_length]])
                        
                        # Charger les scalers sauvegard√©s si pas encore charg√©s
                        if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                            logger.warning("Le scaler_numbers n'est pas fitted. Tentative de chargement depuis le fichier sauvegard√©...")
                            self._load_saved_scalers()
                        
                        if not hasattr(self.scaler_numbers, 'mean_') or self.scaler_numbers.mean_ is None:
                            logger.error("Le scaler_numbers n'est pas fitted et n'a pas pu √™tre charg√©. Impossible de faire des pr√©dictions ML.")
                            logger.warning("Utilisation uniquement des statistiques de fr√©quence pour la pr√©diction.")
                            features_scaled_numbers = None
                            features_scaled_stars = None
                        else:
                            features_scaled_numbers = self.scaler_numbers.transform(features)
                            if hasattr(self, 'scaler_stars') and hasattr(self.scaler_stars, 'mean_') and self.scaler_stars.mean_ is not None:
                                features_scaled_stars = self.scaler_stars.transform(features)
                            else:
                                features_scaled_stars = features_scaled_numbers
                
                # Utiliser les features pr√©par√©es pour la pr√©diction
                if features_scaled_numbers is not None and features_scaled_stars is not None:
                    try:
                        # V√©rifier que le nombre de features correspond
                        if hasattr(self.scaler_numbers, 'n_features_in_') and features_scaled_numbers.shape[1] != self.scaler_numbers.n_features_in_:
                            logger.error(f"‚ùå Nombre de features incompatible: {features_scaled_numbers.shape[1]} features en pr√©diction vs {self.scaler_numbers.n_features_in_} features attendues par le scaler")
                            logger.warning("Utilisation uniquement des statistiques de fr√©quence pour la pr√©diction.")
                        else:
                            # Pr√©dire les probabilit√©s avec les deux mod√®les pour les num√©ros
                            # ‚ö†Ô∏è CRITIQUE : predict_proba pour OneVsRestClassifier retourne un array de shape (n_samples, n_classes)
                            # Chaque colonne correspond √† la probabilit√© de la classe correspondante (num√©ro 1-50)
                            try:
                                gb_number_pred_proba = self.number_model.predict_proba(features_scaled_numbers)
                                rf_number_pred_proba = self.rf_number_model.predict_proba(features_scaled_numbers)
                                
                                # ‚ö†Ô∏è CRITIQUE : Extraire les probabilit√©s correctement selon la forme
                                # OneVsRestClassifier.predict_proba() retourne un array 2D (n_samples, n_classes)
                                if isinstance(gb_number_pred_proba, list):
                                    # Si c'est une liste (format OneVsRestClassifier avec liste), convertir
                                    # Chaque √©l√©ment est un array (n_samples, 2) pour la classe binaire
                                    gb_number_pred_probs = np.array([proba[0][1] if proba.shape[1] > 1 else proba[0][0] for proba in gb_number_pred_proba])
                                elif gb_number_pred_proba.ndim == 2:
                                    # Array 2D : (n_samples, n_classes) - prendre la premi√®re ligne
                                    gb_number_pred_probs = gb_number_pred_proba[0]
                                else:
                                    # Array 1D : utiliser directement
                                    gb_number_pred_probs = gb_number_pred_proba.flatten()
                                
                                if isinstance(rf_number_pred_proba, list):
                                    rf_number_pred_probs = np.array([proba[0][1] if proba.shape[1] > 1 else proba[0][0] for proba in rf_number_pred_proba])
                                elif rf_number_pred_proba.ndim == 2:
                                    rf_number_pred_probs = rf_number_pred_proba[0]
                                else:
                                    rf_number_pred_probs = rf_number_pred_proba.flatten()
                                
                                # ‚ö†Ô∏è CRITIQUE : S'assurer que les arrays ont la m√™me forme et la bonne longueur
                                # Normaliser √† la longueur attendue (max_number = 50)
                                expected_len = self.config["max_number"]
                                if len(gb_number_pred_probs) != expected_len:
                                    logger.warning(f"Longueur GB inattendue: {len(gb_number_pred_probs)} (attendu: {expected_len})")
                                    if len(gb_number_pred_probs) < expected_len:
                                        # Compl√©ter avec des z√©ros
                                        gb_number_pred_probs = np.pad(gb_number_pred_probs, (0, expected_len - len(gb_number_pred_probs)), 'constant')
                                    else:
                                        # Tronquer
                                        gb_number_pred_probs = gb_number_pred_probs[:expected_len]
                                
                                if len(rf_number_pred_probs) != expected_len:
                                    logger.warning(f"Longueur RF inattendue: {len(rf_number_pred_probs)} (attendu: {expected_len})")
                                    if len(rf_number_pred_probs) < expected_len:
                                        rf_number_pred_probs = np.pad(rf_number_pred_probs, (0, expected_len - len(rf_number_pred_probs)), 'constant')
                                    else:
                                        rf_number_pred_probs = rf_number_pred_probs[:expected_len]
                                
                                # Calculer la moyenne des probabilit√©s ML pour les num√©ros
                                avg_ml_prob_numbers = (gb_number_pred_probs + rf_number_pred_probs) / 2.0
                                
                                # Pr√©dire les probabilit√©s avec les deux mod√®les pour les √©toiles
                                gb_star_pred_proba = self.star_model.predict_proba(features_scaled_stars)
                                rf_star_pred_proba = self.rf_star_model.predict_proba(features_scaled_stars)
                                
                                # M√™me traitement pour les √©toiles
                                if isinstance(gb_star_pred_proba, list):
                                    gb_star_pred_probs = np.array([proba[0][1] if proba.shape[1] > 1 else proba[0][0] for proba in gb_star_pred_proba])
                                elif gb_star_pred_proba.ndim == 2:
                                    gb_star_pred_probs = gb_star_pred_proba[0]
                                else:
                                    gb_star_pred_probs = gb_star_pred_proba.flatten()
                                
                                if isinstance(rf_star_pred_proba, list):
                                    rf_star_pred_probs = np.array([proba[0][1] if proba.shape[1] > 1 else proba[0][0] for proba in rf_star_pred_proba])
                                elif rf_star_pred_proba.ndim == 2:
                                    rf_star_pred_probs = rf_star_pred_proba[0]
                                else:
                                    rf_star_pred_probs = rf_star_pred_proba.flatten()
                                
                                # Normaliser √† la longueur attendue pour les √©toiles (max_star = 12)
                                expected_star_len = self.config["max_star"]
                                if len(gb_star_pred_probs) != expected_star_len:
                                    if len(gb_star_pred_probs) < expected_star_len:
                                        gb_star_pred_probs = np.pad(gb_star_pred_probs, (0, expected_star_len - len(gb_star_pred_probs)), 'constant')
                                    else:
                                        gb_star_pred_probs = gb_star_pred_probs[:expected_star_len]
                                
                                if len(rf_star_pred_probs) != expected_star_len:
                                    if len(rf_star_pred_probs) < expected_star_len:
                                        rf_star_pred_probs = np.pad(rf_star_pred_probs, (0, expected_star_len - len(rf_star_pred_probs)), 'constant')
                                    else:
                                        rf_star_pred_probs = rf_star_pred_probs[:expected_star_len]
                                
                                # Calculer la moyenne des probabilit√©s ML pour les √©toiles
                                avg_ml_prob_stars = (gb_star_pred_probs + rf_star_pred_probs) / 2.0
                                
                                # Combiner avec les probabilit√©s existantes
                                weight = self.config["prediction_weight"]
                                for num in range(1, self.config["max_number"] + 1):
                                    idx = num - 1
                                    if idx < len(avg_ml_prob_numbers):
                                        ml_prob = float(avg_ml_prob_numbers[idx])
                                    else:
                                        ml_prob = 0.0
                                    number_probs[num] = number_probs.get(num, 0) * (1 - weight) + ml_prob * weight
                                
                                for star in range(1, self.config["max_star"] + 1):
                                    idx = star - 1
                                    if idx < len(avg_ml_prob_stars):
                                        ml_prob = float(avg_ml_prob_stars[idx])
                                    else:
                                        ml_prob = 0.0
                                    star_probs[star] = star_probs.get(star, 0) * (1 - weight) + ml_prob * weight
                                
                                logger.debug(f"‚úÖ Pr√©dictions ML combin√©es: {len(avg_ml_prob_numbers)} num√©ros, {len(avg_ml_prob_stars)} √©toiles")
                                
                                # 4. Utiliser le pr√©dicteur quantique si disponible
                                if self.quantum_predictor is not None and self.config.get('use_quantum', False):
                                    try:
                                        logger.info("üåå Utilisation du pr√©dicteur quantique pour optimiser la s√©lection...")
                                        # Normaliser les probabilit√©s pour le pr√©dicteur quantique
                                        number_probs_normalized = {k: max(0.0, v) for k, v in number_probs.items()}
                                        star_probs_normalized = {k: max(0.0, v) for k, v in star_probs.items()}
                                        
                                        # Normaliser pour que la somme soit 1
                                        number_sum = sum(number_probs_normalized.values())
                                        star_sum = sum(star_probs_normalized.values())
                                        if number_sum > 0:
                                            number_probs_normalized = {k: v / number_sum for k, v in number_probs_normalized.items()}
                                        if star_sum > 0:
                                            star_probs_normalized = {k: v / star_sum for k, v in star_probs_normalized.items()}
                                        
                                        # Utiliser le pr√©dicteur quantique pour optimiser la s√©lection
                                        quantum_numbers, quantum_stars = self.quantum_predictor.predict(
                                            features=features_scaled_numbers[0] if 'features_scaled_numbers' in locals() else None,
                                            historical_data=self.df,
                                            number_probs=number_probs_normalized,
                                            star_probs=star_probs_normalized
                                        )
                                        
                                        # M√©langer les pr√©dictions quantiques avec les probabilit√©s classiques (poids 0.3 pour le quantique)
                                        quantum_weight = 0.3
                                        for num in quantum_numbers:
                                            number_probs[num] = number_probs.get(num, 0) * (1 - quantum_weight) + quantum_weight
                                        for star in quantum_stars:
                                            star_probs[star] = star_probs.get(star, 0) * (1 - quantum_weight) + quantum_weight
                                        
                                        logger.info(f"‚úÖ Pr√©dictions quantiques appliqu√©es: {quantum_numbers}, {quantum_stars}")
                                    except Exception as e:
                                        logger.warning(f"Erreur lors de l'utilisation du pr√©dicteur quantique: {str(e)}")
                                        logger.debug(traceback.format_exc())
                            except Exception as pred_error:
                                logger.error(f"Erreur lors du calcul des probabilit√©s ML: {str(pred_error)}")
                                logger.debug(traceback.format_exc())
                                # Ne pas lever l'exception, continuer avec les statistiques de fr√©quence
                                logger.warning("Utilisation uniquement des statistiques de fr√©quence pour la pr√©diction.")
                    except Exception as e:
                        logger.error(f"Erreur lors de la pr√©paration des features ML: {str(e)}")
                        logger.debug(traceback.format_exc())
                        logger.warning("Utilisation uniquement des statistiques de fr√©quence pour la pr√©diction.")
                else:
                    logger.warning("Features ML non disponibles. Utilisation uniquement des statistiques de fr√©quence.")
            
            # 4. Ajuster avec les corr√©lations si disponibles
            if self.number_correlations is not None and self.config["use_correlation"]:
                # Obtenir les derniers num√©ros tir√©s
                last_numbers = self.df.iloc[-1][self.number_cols].dropna().astype(int).tolist()
                
                for num in range(1, self.config["max_number"] + 1):
                    # Calculer la corr√©lation moyenne avec les derniers num√©ros tir√©s
                    corr_sum = 0
                    count = 0
                    
                    for last_num in last_numbers:
                        if 1 <= last_num <= self.config["max_number"] and num in self.number_correlations.index and last_num in self.number_correlations.columns:
                            corr_val = self.number_correlations.loc[num, last_num]
                            # Ajouter une pond√©ration pour la corr√©lation (ex: 0.1 pour la corr√©lation)
                            corr_sum += corr_val
                            count += 1
                    
                    if count > 0:
                        avg_corr = corr_sum / count
                        # Ajuster la probabilit√© en fonction de la corr√©lation (normaliser la corr√©lation de -1 √† 1 vers 0 √† 1)
                        # Puis m√©langer avec la probabilit√© existante.
                        # Un poids de 0.2 est utilis√© pour la corr√©lation.
                        number_probs[num] = number_probs.get(num, 0) * (1 - 0.2) + ((avg_corr + 1) / 2) * 0.2
            
            if self.star_correlations is not None and self.config["use_correlation"]:
                last_stars = self.df.iloc[-1][self.star_cols].dropna().astype(int).tolist()
                
                for star in range(1, self.config["max_star"] + 1):
                    corr_sum = 0
                    count = 0
                    
                    for last_star in last_stars:
                        if 1 <= last_star <= self.config["max_star"] and star in self.star_correlations.index and last_star in self.star_correlations.columns:
                            corr_val = self.star_correlations.loc[star, last_star]
                            corr_sum += corr_val
                            count += 1
                    
                    if count > 0:
                        avg_corr = corr_sum / count
                        star_probs[star] = star_probs.get(star, 0) * (1 - 0.2) + ((avg_corr + 1) / 2) * 0.2
            
            # 5. Appliquer la pond√©ration Fibonacci invers√©e si activ√©e
            if self.config.get("use_fibonacci_inverse", False):
                logger.info("Application de la pond√©ration Fibonacci invers√©e...")
                
                # Cr√©er des compteurs √† partir des probabilit√©s actuelles
                # Utilise les probabilit√©s existantes pour d√©terminer l'ordre de pond√©ration
                # Multiplier par 1000 pour donner une base enti√®re pour Counter
                number_counter = Counter({num: int(prob * 1000) for num, prob in number_probs.items()})
                star_counter = Counter({star: int(prob * 1000) for star, prob in star_probs.items()})
                
                # Appliquer la pond√©ration Fibonacci invers√©e
                # reverse_order=True signifie que les √©l√©ments avec les plus petites "fr√©quences" (probabilit√©s ici)
                # recevront les poids Fibonacci les plus √©lev√©s.
                fibonacci_number_weights = apply_inverse_fibonacci_weights(number_counter, reverse_order=True)
                fibonacci_star_weights = apply_inverse_fibonacci_weights(star_counter, reverse_order=True)
                
                # Obtenir le poids de m√©lange depuis la configuration
                blend_weight = self.config.get("fibonacci_inverse_weight_blend", 0.5) # Par d√©faut 0.5 si non sp√©cifi√©
                
                # Combiner avec les probabilit√©s existantes
                for num in range(1, self.config["max_number"] + 1):
                    fib_weight = fibonacci_number_weights.get(num, 0.0) # Assurer que c'est un float
                    # Nouvelle probabilit√© = (Probabilit√© actuelle * (1 - poids de m√©lange)) + (Poids Fibonacci * poids de m√©lange)
                    number_probs[num] = number_probs.get(num, 0.0) * (1 - blend_weight) + fib_weight * blend_weight
                
                for star in range(1, self.config["max_star"] + 1):
                    fib_weight = fibonacci_star_weights.get(star, 0.0) # Assurer que c'est un float
                    star_probs[star] = star_probs.get(star, 0.0) * (1 - blend_weight) + fib_weight * blend_weight
                
                logger.info(f"Pond√©ration Fibonacci invers√©e appliqu√©e (poids de m√©lange: {blend_weight})")
            
            # 6. S√©lectionner les num√©ros et √©toiles avec les probabilit√©s les plus √©lev√©es
            # Normaliser les probabilit√©s pour qu'elles somment √† 1, si n√©cessaire pour la s√©lection pond√©r√©e
            total_num_prob = sum(number_probs.values())
            if total_num_prob > 0:
                number_probs = {num: prob / total_num_prob for num, prob in number_probs.items()}
            
            total_star_prob = sum(star_probs.values())
            if total_star_prob > 0:
                star_probs = {star: prob / total_star_prob for star, prob in star_probs.items()}

            # Stocker les pr√©dictions (probabilit√©s finales)
            self.number_predictions = number_probs
            self.star_predictions = star_probs

            # S√©lection des num√©ros et √©toiles pr√©dits (les plus probables)
            sorted_numbers = sorted(number_probs.items(), key=lambda x: x[1], reverse=True)
            sorted_stars = sorted(star_probs.items(), key=lambda x: x[1], reverse=True)
            
            predicted_numbers = [num for num, _ in sorted_numbers[:self.config["propose_size"]]]
            predicted_stars = [star for star, _ in sorted_stars[:self.config["star_size"]]]
            
            logger.info(f"Pr√©diction: num√©ros {predicted_numbers}, √©toiles {predicted_stars}")
            
            return predicted_numbers, predicted_stars
            
        except Exception as e:
            logger.error(f"Erreur lors de la pr√©diction des num√©ros: {str(e)}")
            logger.debug(traceback.format_exc())
            return [], []
    
    def predict_next_draw(self) -> Tuple[List[int], List[int]]:
        """
        Pr√©dit les num√©ros et les √©toiles pour le prochain tirage.
        Alias pour predict_numbers.
        
        Returns:
            Tuple contenant:
            - Liste des num√©ros pr√©dits
            - Liste des √©toiles pr√©dites
        """
        return self.predict_numbers()
    
    def generate_combinations(self, number_scores: Dict[int, float], star_scores: Dict[int, float]) -> List[Tuple[List[int], List[int]]]:
        """
        G√©n√®re plusieurs combinaisons optimis√©es pour le prochain tirage,
        en utilisant les scores finaux fournis.
        
        Args:
            number_scores (Dict[int, float]): Scores finaux pour les num√©ros.
            star_scores (Dict[int, float]): Scores finaux pour les √©toiles.

        Returns:
            Liste de tuples, chaque tuple contenant:
            - Liste des num√©ros
            - Liste des √©toiles
        """
        logger.info(f"G√©n√©ration de {self.config['combinations_to_generate']} combinaisons optimis√©es...")
        
        try:
            combinations = []
            
            # Strat√©gie 1: Utiliser les num√©ros chauds et froids
            # S'assurer que les listes hot/cold ne sont pas vides
            hot_numbers = self.hot[:self.config["propose_size"]] if self.hot else []
            cold_numbers = self.cold[:self.config["propose_size"]] if self.cold else []
            
            hot_stars = self.star_hot[:self.config["star_size"]] if self.star_hot else []
            cold_stars = self.star_cold[:self.config["star_size"]] if self.star_cold else []
            
            # Compl√©ter si n√©cessaire avec des num√©ros/√©toiles al√©atoires mais valides
            def complete_list(current_list, max_val, target_size):
                if len(current_list) < target_size:
                    available = list(set(range(1, max_val + 1)) - set(current_list))
                    if len(available) > 0:
                        current_list.extend(random.sample(available, min(target_size - len(current_list), len(available))))
                return current_list

            hot_numbers = complete_list(hot_numbers, self.config["max_number"], self.config["propose_size"])
            cold_numbers = complete_list(cold_numbers, self.config["max_number"], self.config["propose_size"])
            hot_stars = complete_list(hot_stars, self.config["max_star"], self.config["star_size"])
            cold_stars = complete_list(cold_stars, self.config["max_star"], self.config["star_size"])

            # Ajouter les combinaisons de base (si compl√®tes)
            if len(hot_numbers) == self.config["propose_size"] and len(hot_stars) == self.config["star_size"]:
                combinations.append((sorted(hot_numbers), sorted(hot_stars)))
            if len(cold_numbers) == self.config["propose_size"] and len(cold_stars) == self.config["star_size"]:
                combinations.append((sorted(cold_numbers), sorted(cold_stars)))
            
            # Strat√©gie 2: M√©langer chauds et froids
            if len(hot_numbers) >= 3 and len(cold_numbers) >= 2 and len(hot_stars) >= 1 and len(cold_stars) >= 1:
                mixed_numbers = random.sample(hot_numbers, 3) + random.sample(cold_numbers, 2)
                mixed_stars = random.sample(hot_stars, 1) + random.sample(cold_stars, 1)
                combinations.append((sorted(mixed_numbers), sorted(mixed_stars)))
            
            # Strat√©gie 3: Utiliser les pr√©dictions directes (d√©j√† pond√©r√©es par ML, Fibonacci, etc.)
            predicted_numbers, predicted_stars = self.predict_next_draw() # Utilise predict_next_draw qui est un alias
            if predicted_numbers and predicted_stars:
                combinations.append((sorted(predicted_numbers), sorted(predicted_stars)))
            
            # Strat√©gie 4: Simulation Monte Carlo avec filtrage
            logger.info("D√©marrage de la simulation Monte Carlo pour la g√©n√©ration de combinaisons...")
            monte_carlo_combinations = self._run_monte_carlo_simulation(number_scores, star_scores)
            
            # Ajouter les combinaisons filtr√©es de Monte Carlo
            combinations.extend(monte_carlo_combinations)
            
            # Strat√©gie 5: Syst√®me r√©ducteur (Wheeling System)
            # Utiliser les scores pour s√©lectionner les num√©ros/√©toiles √† inclure dans le syst√®me r√©ducteur
            if self.config.get("use_wheeling_system", True): # Ajouter au config si besoin
                logger.info("G√©n√©ration de combinaisons via syst√®me r√©ducteur...")
                
                num_to_wheel_count = self.config.get("wheeling_num_count", 10)
                star_to_wheel_count = self.config.get("wheeling_star_count", 5)
                
                if number_scores and star_scores:
                    # S√©lectionner les num√©ros/√©toiles les plus prometteurs selon les scores combin√©s
                    numbers_to_wheel = sorted(number_scores, key=number_scores.get, reverse=True)[:num_to_wheel_count]
                    stars_to_wheel = sorted(star_scores, key=star_scores.get, reverse=True)[:star_to_wheel_count]
                    
                    wheeled_combinations = self._generate_wheeling_combinations(numbers_to_wheel, stars_to_wheel)
                    combinations.extend(wheeled_combinations)
                else:
                    logger.warning("Scores non disponibles pour la s√©lection des num√©ros/√©toiles pour le syst√®me r√©ducteur.")
            
            # S'assurer qu'on a le bon nombre de combinaisons uniques
            unique_combinations = []
            seen_combos = set()
            for nums, stars in combinations:
                combo_tuple = (tuple(sorted(nums)), tuple(sorted(stars)))
                if combo_tuple not in seen_combos:
                    unique_combinations.append((sorted(nums), sorted(stars)))
                    seen_combos.add(combo_tuple)
            
            # Si pas assez de combinaisons uniques, compl√©ter avec des al√©atoires pond√©r√©es (sans filtre avanc√©)
            # Cette boucle est une s√©curit√© pour atteindre le nombre d√©sir√©
            while len(unique_combinations) < self.config["combinations_to_generate"]:
                logger.warning(f"Pas assez de combinaisons uniques ({len(unique_combinations)}), ajout de combinaisons al√©atoires pond√©r√©es suppl√©mentaires.")
                nums, stars = self._generate_weighted_random_combination(number_scores, star_scores)
                combo_tuple = (tuple(sorted(nums)), tuple(sorted(stars)))
                if combo_tuple not in seen_combos:
                    unique_combinations.append((sorted(nums), sorted(stars)))
                    seen_combos.add(combo_tuple)
                # Ajouter une petite s√©curit√© pour √©viter boucle infinie si scores tr√®s concentr√©s
                if len(seen_combos) > self.config["monte_carlo_simulations"] * 2: # Limite d'essais pour la compl√©tion
                    logger.error("Impossible de g√©n√©rer suffisamment de combinaisons uniques, arr√™t.")
                    break

            # S√©lectionner le nombre final de combinaisons
            final_combinations = unique_combinations[:self.config["combinations_to_generate"]]

            # Enregistrer l'historique des combinaisons g√©n√©r√©es
            current_date = datetime.now().strftime("%Y-%m-%d") # Ou utiliser la date du dernier tirage + 1 ?
            self.generated_combinations_history.append((current_date, final_combinations))
            # Limiter la taille de l'historique si n√©cessaire
            max_history = self.config.get("max_combination_history", 100)
            if len(self.generated_combinations_history) > max_history:
                self.generated_combinations_history.pop(0)

            logger.info(f"{len(final_combinations)} combinaisons g√©n√©r√©es et filtr√©es")
            
            return final_combinations
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des combinaisons: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
    
    def run_analysis(self) -> bool:
        """
        Ex√©cute l'analyse compl√®te.
        
        Returns:
            bool: True si l'analyse a r√©ussi, False sinon
        """
        logger.info("D√©marrage de l'analyse avanc√©e des tirages d'Euromillions...")
        
        try:
            # 1. Chargement des donn√©es
            if not self.load_data():
                return False
            
            # 2. Calcul des statistiques globales
            self.compute_global_stats()
            
            # 3. Calcul des statistiques sur la fen√™tre r√©cente
            self.compute_window_stats()
            
            # 4. Identification des num√©ros chauds et froids
            self.identify_hot_cold()
            
            # 5. Analyse des corr√©lations
            if self.config["use_correlation"]:
                self.analyze_correlations()
            
            # 6. Analyse des tendances temporelles
            if self.config["use_temporal"]:
                self.analyze_temporal_patterns()
            
            # 7. Analyse des clusters
            if self.config["use_clustering"]:
                self.analyze_clustering()
            
            # 8. Analyses suppl√©mentaires
            if self.config["analyze_parity"]:
                self.analyze_parity()
            
            if self.config["analyze_sum"]:
                self.analyze_sum()
            
            self.analyze_sequences()

            # 9. Calcul des scores d'√©cart (n√©cessaire avant compute_scores)
            self.compute_gap_scores()
            
            # 10. Entra√Ænement des mod√®les ML
            if self.config["use_ml"]:
                self.train_ml_models()
            
            # 11. Pr√©diction des num√©ros (g√©n√®re self.number_predictions et self.star_predictions)
            self.predict_numbers()

            # 12. Calcul des scores finaux combin√©s
            self.final_number_scores, self.final_star_scores = self.compute_scores()
            
            logger.info("Analyse termin√©e avec succ√®s")
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'analyse: {str(e)}")
            logger.debug(traceback.format_exc())
            return False
    
    def generate_report(self) -> str:
        """
        G√©n√®re un rapport d√©taill√© de l'analyse.
        
        Returns:
            str: Chemin du fichier de rapport
        """
        logger.info("G√©n√©ration du rapport d'analyse...")
        
        try:
            # Cr√©er le r√©pertoire de sortie s'il n'existe pas
            if not self.output_dir.exists():
                self.output_dir.mkdir(parents=True)
            
            # Nom du fichier de rapport
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.output_dir / f"rapport_euromillions_{timestamp}.txt"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("RAPPORT D'ANALYSE AVANC√âE DES TIRAGES EUROMILLIONS\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"Date de l'analyse: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Fichier analys√©: {self.config['csv_file']}\n")
                f.write(f"Nombre de tirages: {len(self.df)}\n\n")
                
                # Statistiques globales
                f.write("-" * 80 + "\n")
                f.write("STATISTIQUES GLOBALES\n")
                f.write("-" * 80 + "\n\n")
                
                f.write("Fr√©quence des num√©ros principaux:\n")
                if self.freq:
                    for num, count in sorted(self.freq.items()):
                        f.write(f"  {num}: {count} fois ({count/(len(self.df)*5)*100:.2f}%)\n")
                
                f.write("\nFr√©quence des √©toiles:\n")
                if self.star_freq:
                    for star, count in sorted(self.star_freq.items()):
                        f.write(f"  {star}: {count} fois ({count/(len(self.df)*2)*100:.2f}%)\n")
                
                # Num√©ros chauds et froids
                f.write("\n" + "-" * 80 + "\n")
                f.write("NUM√âROS CHAUDS ET FROIDS\n")
                f.write("-" * 80 + "\n\n")
                
                f.write(f"Num√©ros chauds (les plus fr√©quents sur les {self.config['window_draws']} derniers tirages):\n")
                if self.hot:
                    for num in self.hot:
                        count = self.window_counts.get(num, 0)
                        f.write(f"  {num}: {count} fois ({count/(min(self.config['window_draws'], len(self.df))*5)*100:.2f}%)\n")
                
                f.write(f"\nNum√©ros froids (les moins fr√©quents sur les {self.config['window_draws']} derniers tirages):\n")
                if self.cold:
                    for num in self.cold:
                        count = self.window_counts.get(num, 0)
                        f.write(f"  {num}: {count} fois ({count/(min(self.config['window_draws'], len(self.df))*5)*100:.2f}%)\n")
                
                f.write(f"\n√âtoiles chaudes (les plus fr√©quentes sur les {self.config['window_draws']} derniers tirages):\n")
                if self.star_hot:
                    for star in self.star_hot:
                        count = self.star_window_counts.get(star, 0)
                        f.write(f"  {star}: {count} fois ({count/(min(self.config['window_draws'], len(self.df))*2)*100:.2f}%)\n")
                
                f.write(f"\n√âtoiles froides (les moins fr√©quentes sur les {self.config['window_draws']} derniers tirages):\n")
                if self.star_cold:
                    for star in self.star_cold:
                        count = self.star_window_counts.get(star, 0)
                        f.write(f"  {star}: {count} fois ({count/(min(self.config['window_draws'], len(self.df))*2)*100:.2f}%)\n")
                
                # Analyses suppl√©mentaires
                if self.parity_stats:
                    f.write("\n" + "-" * 80 + "\n")
                    f.write("ANALYSE DE PARIT√â\n")
                    f.write("-" * 80 + "\n\n")
                    
                    f.write("Distribution des num√©ros pairs/impairs:\n")
                    for pattern, stats in sorted(self.parity_stats.items()):
                        f.write(f"  {pattern}: {stats['count']} fois ({stats['percentage']:.2f}%)\n")
                
                if self.sum_stats:
                    f.write("\n" + "-" * 80 + "\n")
                    f.write("ANALYSE DE SOMME\n")
                    f.write("-" * 80 + "\n\n")
                    
                    f.write(f"Somme minimale: {self.sum_stats['min']}\n")
                    f.write(f"Somme maximale: {self.sum_stats['max']}\n")
                    f.write(f"Somme moyenne: {self.sum_stats['avg']:.2f}\n")
                    f.write(f"Somme m√©diane: {self.sum_stats['median']:.2f}\n")
                    f.write(f"√âcart-type: {self.sum_stats['std']:.2f}\n\n")
                    
                    f.write("Distribution des sommes par plage:\n")
                    for range_key, stats in sorted(self.sum_ranges.items()):
                        f.write(f"  {range_key}: {stats['count']} fois ({stats['percentage']:.2f}%)\n")
                    
                    f.write("\nSommes les plus fr√©quentes:\n")
                    for sum_val, count in self.most_common_sums:
                        f.write(f"  {sum_val}: {count} fois\n")
                
                if self.sequence_stats:
                    f.write("\n" + "-" * 80 + "\n")
                    f.write("ANALYSE DES S√âQUENCES\n")
                    f.write("-" * 80 + "\n\n")
                    
                    f.write("Distribution des s√©quences de num√©ros cons√©cutifs:\n")
                    for seq_count, stats in sorted(self.sequence_stats.items()):
                        f.write(f"  {seq_count} s√©quence(s): {stats['count']} fois ({stats['percentage']:.2f}%)\n")
                
                # Pr√©dictions
                f.write("\n" + "-" * 80 + "\n")
                f.write("PR√âDICTIONS POUR LE PROCHAIN TIRAGE\n")
                f.write("-" * 80 + "\n\n")
                
                # Utiliser les scores finaux pour la pr√©diction affich√©e
                predicted_numbers = sorted(self.final_number_scores, key=self.final_number_scores.get, reverse=True)[:self.config["propose_size"]]
                predicted_stars = sorted(self.final_star_scores, key=self.final_star_scores.get, reverse=True)[:self.config["star_size"]]
                
                f.write(f"Num√©ros pr√©dits: {', '.join(map(str, predicted_numbers))}\n")
                f.write(f"√âtoiles pr√©dites: {', '.join(map(str, predicted_stars))}\n\n")
                
                f.write("Top 10 num√©ros avec leurs scores finaux:\n")
                if self.final_number_scores:
                    sorted_numbers = sorted(self.final_number_scores.items(), key=lambda x: x[1], reverse=True)[:10]
                    for num, score in sorted_numbers:
                        f.write(f"  {num}: {score:.4f}\n")
                
                f.write("\nTop 5 √©toiles avec leurs scores finaux:\n")
                if self.final_star_scores:
                    sorted_stars = sorted(self.final_star_scores.items(), key=lambda x: x[1], reverse=True)[:5]
                    for star, score in sorted_stars:
                        f.write(f"  {star}: {score:.4f}\n")
                
                # Combinaisons optimis√©es
                f.write("\n" + "-" * 80 + "\n")
                f.write("COMBINAISONS OPTIMIS√âES\n")
                f.write("-" * 80 + "\n\n")
                
                # Passer les scores finaux √† generate_combinations
                combinations = self.generate_combinations(self.final_number_scores, self.final_star_scores)
                
                for i, (numbers, stars) in enumerate(combinations):
                    f.write(f"Combinaison {i+1}: {' - '.join(map(str, numbers))} | {' - '.join(map(str, stars))}\n")
                
                f.write("\n" + "=" * 80 + "\n")
                f.write("FIN DU RAPPORT\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"Rapport g√©n√©r√©: {report_file}")
            
            return str(report_file)
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du rapport: {str(e)}")
            logger.debug(traceback.format_exc())
            return ""
    
    def generate_visualizations(self) -> List[str]:
        """
        G√©n√®re des visualisations des r√©sultats de l'analyse.
        
        Returns:
            List[str]: Liste des chemins des fichiers de visualisation
        """
        logger.info("G√©n√©ration des visualisations...")
        
        try:
            # Cr√©er le r√©pertoire de visualisations
            vis_dir = self.output_dir / "visualizations"
            if not vis_dir.exists():
                vis_dir.mkdir(parents=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_files = []
            
            # 1. Distribution des fr√©quences des num√©ros
            plt.figure(figsize=(12, 8))
            
            if self.freq:
                nums = list(range(1, self.config["max_number"] + 1))
                freqs = [self.freq.get(num, 0) for num in nums]
                
                plt.bar(nums, freqs, color='royalblue')
                plt.axhline(y=sum(freqs) / len(nums), color='r', linestyle='-', label='Moyenne')
                
                plt.title('Distribution des fr√©quences des num√©ros principaux')
                plt.xlabel('Num√©ro')
                plt.ylabel('Fr√©quence')
                plt.xticks(nums[::5])
                plt.grid(axis='y', alpha=0.3)
                plt.legend()
                
                plot_path = vis_dir / f"number_frequency_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 2. Distribution des fr√©quences des √©toiles
            plt.figure(figsize=(10, 6))
            
            if self.star_freq:
                stars = list(range(1, self.config["max_star"] + 1))
                star_freqs = [self.star_freq.get(star, 0) for star in stars]
                
                plt.bar(stars, star_freqs, color='gold')
                plt.axhline(y=sum(star_freqs) / len(stars), color='r', linestyle='-', label='Moyenne')
                
                plt.title('Distribution des fr√©quences des √©toiles')
                plt.xlabel('√âtoile')
                plt.ylabel('Fr√©quence')
                plt.xticks(stars)
                plt.grid(axis='y', alpha=0.3)
                plt.legend()
                
                plot_path = vis_dir / f"star_frequency_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 3. Num√©ros chauds et froids
            plt.figure(figsize=(12, 8))
            
            if self.window_counts:
                all_nums = list(range(1, self.config["max_number"] + 1))
                all_freqs = [self.window_counts.get(num, 0) for num in all_nums]
                
                colors = ['royalblue'] * self.config["max_number"]
                
                # Marquer les num√©ros chauds en rouge
                if self.hot:
                    for num in self.hot:
                        if 1 <= num <= self.config["max_number"]:
                            colors[num-1] = 'crimson'
                
                # Marquer les num√©ros froids en bleu clair
                if self.cold:
                    for num in self.cold:
                        if 1 <= num <= self.config["max_number"]:
                            colors[num-1] = 'skyblue'
                
                plt.bar(all_nums, all_freqs, color=colors)
                
                plt.title(f'Num√©ros chauds et froids (sur les {self.config["window_draws"]} derniers tirages)')
                plt.xlabel('Num√©ro')
                plt.ylabel('Fr√©quence')
                plt.xticks(all_nums[::5])
                plt.grid(axis='y', alpha=0.3)
                
                # L√©gende personnalis√©e
                from matplotlib.patches import Patch
                legend_elements = [
                    Patch(facecolor='crimson', label='Num√©ros chauds'),
                    Patch(facecolor='skyblue', label='Num√©ros froids'),
                    Patch(facecolor='royalblue', label='Autres num√©ros')
                ]
                plt.legend(handles=legend_elements)
                
                plot_path = vis_dir / f"hot_cold_numbers_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 4. Analyse de parit√©
            if self.parity_stats:
                plt.figure(figsize=(10, 6))
                
                patterns = list(self.parity_stats.keys())
                counts = [self.parity_stats[p]['count'] for p in patterns]
                
                plt.bar(patterns, counts, color='mediumseagreen')
                
                plt.title('Distribution des combinaisons de parit√©')
                plt.xlabel('Combinaison (Pairs-Impairs)')
                plt.ylabel('Nombre de tirages')
                plt.grid(axis='y', alpha=0.3)
                
                plot_path = vis_dir / f"parity_distribution_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 5. Analyse de somme
            if self.sum_stats and self.sum_ranges:
                plt.figure(figsize=(12, 6))
                
                ranges = list(self.sum_ranges.keys())
                counts = [self.sum_ranges[r]['count'] for r in ranges]
                
                plt.bar(ranges, counts, color='purple')
                
                plt.title('Distribution des sommes par plage')
                plt.xlabel('Plage de somme')
                plt.ylabel('Nombre de tirages')
                plt.xticks(rotation=45)
                plt.grid(axis='y', alpha=0.3)
                
                plot_path = vis_dir / f"sum_distribution_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 6. Pr√©dictions
            if self.number_predictions and self.star_predictions:
                plt.figure(figsize=(14, 10))
                
                # Sous-graphique pour les num√©ros
                plt.subplot(2, 1, 1)
                
                sorted_numbers = sorted(self.number_predictions.items(), key=lambda x: x[1], reverse=True)[:15]
                nums = [num for num, _ in sorted_numbers]
                probs = [prob for _, prob in sorted_numbers]
                
                bars = plt.bar(nums, probs, color='royalblue')
                
                # Marquer les num√©ros pr√©dits
                # Utiliser les scores finaux pour la s√©lection des num√©ros pr√©dits
                predicted_numbers = sorted(self.final_number_scores, key=self.final_number_scores.get, reverse=True)[:self.config["propose_size"]]
                for i, num in enumerate(nums):
                    if num in predicted_numbers:
                        bars[i].set_color('crimson')
                
                plt.title('Probabilit√©s des num√©ros principaux (top 15)')
                plt.xlabel('Num√©ro')
                plt.ylabel('Probabilit√©')
                plt.grid(axis='y', alpha=0.3)
                
                # Sous-graphique pour les √©toiles
                plt.subplot(2, 1, 2)
                
                sorted_stars = sorted(self.star_predictions.items(), key=lambda x: x[1], reverse=True)
                stars = [star for star, _ in sorted_stars]
                star_probs = [prob for _, prob in sorted_stars]
                
                bars = plt.bar(stars, star_probs, color='gold')
                
                # Marquer les √©toiles pr√©dites
                # Utiliser les scores finaux pour la s√©lection des √©toiles pr√©dites
                predicted_stars = sorted(self.final_star_scores, key=self.final_star_scores.get, reverse=True)[:self.config["star_size"]]
                for i, star in enumerate(stars):
                    if star in predicted_stars:
                        bars[i].set_color('crimson')
                
                plt.title('Probabilit√©s des √©toiles')
                plt.xlabel('√âtoile')
                plt.ylabel('Probabilit√©')
                plt.grid(axis='y', alpha=0.3)
                
                plot_path = vis_dir / f"predictions_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            # 7. √âvolution temporelle si disponible
            if self.temporal_patterns and 'Date' in self.df.columns:
                plt.figure(figsize=(14, 8))
                
                # S√©lectionner quelques num√©ros repr√©sentatifs
                if self.hot and self.cold:
                    selected_nums = self.hot[:3] + self.cold[:2]
                else:
                    selected_nums = list(range(1, 6))
                
                for num in selected_nums:
                    if num in self.temporal_patterns:
                        periods = [str(p) for p, _ in self.temporal_patterns[num]]
                        freqs = [f for _, f in self.temporal_patterns[num]]
                        
                        plt.plot(periods, freqs, marker='o', label=f'Num√©ro {num}')
                
                plt.title('√âvolution temporelle des fr√©quences')
                plt.xlabel('P√©riode')
                plt.ylabel('Fr√©quence relative')
                plt.xticks(rotation=45)
                plt.grid(alpha=0.3)
                plt.legend()
                
                plot_path = vis_dir / f"temporal_evolution_{timestamp}.png"
                plt.tight_layout()
                plt.savefig(plot_path)
                plt.close()
                
                plot_files.append(str(plot_path))
            
            logger.info(f"{len(plot_files)} visualisations g√©n√©r√©es")
            
            return plot_files
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des visualisations: {str(e)}")
            logger.debug(traceback.format_exc())
            return []
    
    def export_to_excel(self) -> str:
        """
        Exporte les r√©sultats de l'analyse vers un fichier Excel.
        
        Returns:
            str: Chemin du fichier Excel
        """
        if not self.config["export_excel"]:
            return ""
            
        logger.info("Export des r√©sultats vers Excel...")
        
        try:
            # V√©rifier si pandas a la fonctionnalit√© d'export Excel
            if not hasattr(pd.DataFrame, 'to_excel'):
                logger.error("Fonctionnalit√© d'export Excel non disponible dans pandas")
                return ""
            
            # Cr√©er le r√©pertoire de sortie s'il n'existe pas
            if not self.output_dir.exists():
                self.output_dir.mkdir(parents=True)
            
            # Nom du fichier Excel
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_file = self.output_dir / f"euromillions_analysis_{timestamp}.xlsx"
            
            # Cr√©er un writer Excel
            with pd.ExcelWriter(excel_file) as writer:
                # Feuille 1: Statistiques des num√©ros
                if self.freq:
                    df_numbers = pd.DataFrame({
                        'Num√©ro': list(range(1, self.config["max_number"] + 1)),
                        'Fr√©quence globale': [self.freq.get(num, 0) for num in range(1, self.config["max_number"] + 1)],
                        'Fr√©quence r√©cente': [self.window_counts.get(num, 0) if self.window_counts else 0 for num in range(1, self.config["max_number"] + 1)],
                        'Score final pr√©dit': [self.final_number_scores.get(num, 0) if hasattr(self, 'final_number_scores') and self.final_number_scores else 0 for num in range(1, self.config["max_number"] + 1)]
                    })
                    
                    # Ajouter des colonnes pour les num√©ros chauds/froids
                    df_numbers['Est chaud'] = df_numbers['Num√©ro'].apply(lambda x: 'Oui' if self.hot and x in self.hot else 'Non')
                    df_numbers['Est froid'] = df_numbers['Num√©ro'].apply(lambda x: 'Oui' if self.cold and x in self.cold else 'Non')
                    
                    df_numbers.to_excel(writer, sheet_name='Statistiques Num√©ros', index=False)
                
                # Feuille 2: Statistiques des √©toiles
                if self.star_freq:
                    df_stars = pd.DataFrame({
                        '√âtoile': list(range(1, self.config["max_star"] + 1)),
                        'Fr√©quence globale': [self.star_freq.get(star, 0) for star in range(1, self.config["max_star"] + 1)],
                        'Fr√©quence r√©cente': [self.star_window_counts.get(star, 0) if self.star_window_counts else 0 for star in range(1, self.config["max_star"] + 1)],
                        'Score final pr√©dit': [self.final_star_scores.get(star, 0) if hasattr(self, 'final_star_scores') and self.final_star_scores else 0 for star in range(1, self.config["max_star"] + 1)]
                    })
                    
                    df_stars['Est chaude'] = df_stars['√âtoile'].apply(lambda x: 'Oui' if self.star_hot and x in self.star_hot else 'Non')
                    df_stars['Est froide'] = df_stars['√âtoile'].apply(lambda x: 'Oui' if self.star_cold and x in self.star_cold else 'Non')
                    
                    df_stars.to_excel(writer, sheet_name='Statistiques √âtoiles', index=False)
                
                # Feuille 3: Pr√©dictions
                # Utiliser les scores finaux pour les num√©ros et √©toiles pr√©dits
                predicted_numbers = sorted(self.final_number_scores, key=self.final_number_scores.get, reverse=True)[:self.config["propose_size"]]
                predicted_stars = sorted(self.final_star_scores, key=self.final_star_scores.get, reverse=True)[:self.config["star_size"]]

                combinations = self.generate_combinations(self.final_number_scores, self.final_star_scores)
                
                df_predictions = pd.DataFrame({
                    'Num√©ros pr√©dits': [', '.join(map(str, predicted_numbers))],
                    '√âtoiles pr√©dites': [', '.join(map(str, predicted_stars))]
                })
                
                df_predictions.to_excel(writer, sheet_name='Pr√©dictions', index=False)
                
                # Feuille 4: Combinaisons optimis√©es
                if combinations:
                    df_combinations = pd.DataFrame({
                        'Combinaison': [f"{i+1}" for i in range(len(combinations))],
                        'Num√©ros': [', '.join(map(str, nums)) for nums, _ in combinations],
                        '√âtoiles': [', '.join(map(str, stars)) for _, stars in combinations]
                    })
                    
                    df_combinations.to_excel(writer, sheet_name='Combinaisons', index=False)
                
                # Feuille 5: Analyses suppl√©mentaires
                if self.parity_stats or self.sum_stats or self.sequence_stats:
                    data = {}
                    
                    if self.parity_stats:
                        data['Distribution de parit√©'] = [f"{pattern}: {stats['count']} ({stats['percentage']:.2f}%)" for pattern, stats in self.parity_stats.items()]
                    
                    if self.sum_stats:
                        data['Statistiques de somme'] = [
                            f"Min: {self.sum_stats['min']}",
                            f"Max: {self.sum_stats['max']}",
                            f"Moyenne: {self.sum_stats['avg']:.2f}",
                            f"M√©diane: {self.sum_stats['median']:.2f}",
                            f"√âcart-type: {self.sum_stats['std']:.2f}"
                        ]
                        
                        if self.sum_ranges:
                            data['Distribution des sommes'] = [f"{range_key}: {stats['count']} ({stats['percentage']:.2f}%)" for range_key, stats in self.sum_ranges.items()]
                    
                    if self.sequence_stats:
                        data['Distribution des s√©quences'] = [f"{seq_count} s√©quence(s): {stats['count']} ({stats['percentage']:.2f}%)" for seq_count, stats in self.sequence_stats.items()]
                    
                    # Trouver la longueur maximale
                    max_len = max(len(values) for values in data.values())
                    
                    # Compl√©ter les listes plus courtes
                    for key in data:
                        data[key] = data[key] + [''] * (max_len - len(data[key]))
                    
                    df_analyses = pd.DataFrame(data)
                    df_analyses.to_excel(writer, sheet_name='Analyses suppl√©mentaires', index=False)
                
                # Feuille 6: Donn√©es brutes
                if self.df is not None:
                    self.df.to_excel(writer, sheet_name='Donn√©es brutes', index=False)
            
            logger.info(f"Export Excel termin√©: {excel_file}")
            
            return str(excel_file)
            
        except Exception as e:
            logger.error(f"Erreur lors de l'export Excel: {str(e)}")
            logger.debug(traceback.format_exc())
            return ""
    
    def backtesting(self, start_idx=None, end_idx=None, step_size=1):
        """
        Effectue un backtesting des pr√©dictions sur les donn√©es historiques.
        
        Args:
            start_idx: Indice de d√©but pour le backtesting (None = d√©but des donn√©es)
            end_idx: Indice de fin pour le backtesting (None = fin des donn√©es)
            step_size: Nombre de tirages √† avancer √† chaque √©tape
            
        Returns:
            Dict: R√©sultats du backtesting
        """
        logger.info("D√©marrage du backtesting...")
        
        try:
            if self.df is None or len(self.df) < 10:
                logger.error("Donn√©es insuffisantes pour le backtesting")
                return {}
            
            # D√©finir les indices par d√©faut
            if start_idx is None:
                start_idx = 0
            
            if end_idx is None:
                end_idx = len(self.df)
            
            # V√©rifier les limites
            if start_idx < 0:
                start_idx = 0
            
            if end_idx > len(self.df):
                end_idx = len(self.df)
            
            if start_idx >= end_idx:
                logger.error("Indices de backtesting invalides")
                return {}
            
            # Initialiser les r√©sultats
            results = {
                'correct_numbers': [],
                'correct_stars': [],
                'accuracy_numbers': [],
                'accuracy_stars': [],
                'predictions': []
            }
            
            # Fen√™tre minimale pour l'entra√Ænement
            min_window = 20
            
            # Boucle de backtesting
            for i in range(start_idx + min_window, end_idx, step_size):
                logger.info(f"Backtesting: tirage {i+1}/{end_idx}")
                
                # Cr√©er un sous-ensemble des donn√©es jusqu'√† l'indice i (exclus)
                train_df = self.df.iloc[:i].copy()
                
                # Cr√©er un analyseur temporaire
                temp_config = self.config.copy()
                temp_config["window_draws"] = min(50, len(train_df))
                
                temp_analyzer = EuromillionsAdvancedAnalyzer(temp_config)
                temp_analyzer.df = train_df
                temp_analyzer.number_cols = self.number_cols
                temp_analyzer.star_cols = self.star_cols
                
                # Ex√©cuter l'analyse
                temp_analyzer.compute_global_stats()
                temp_analyzer.compute_window_stats()
                temp_analyzer.identify_hot_cold()
                
                if self.config["use_correlation"]:
                    temp_analyzer.analyze_correlations()
                
                if self.config["use_temporal"]:
                    temp_analyzer.analyze_temporal_patterns()
                
                if self.config["use_clustering"]:
                    temp_analyzer.analyze_clustering()
                
                # Calculer les scores d'√©cart pour le temp_analyzer
                temp_analyzer.compute_gap_scores()

                if self.config["use_ml"]:
                    temp_analyzer.train_ml_models()
                
                # Pr√©dire le prochain tirage (met √† jour number_predictions et star_predictions)
                temp_analyzer.predict_numbers()

                # Calculer les scores finaux combin√©s pour le temp_analyzer
                temp_number_scores, temp_star_scores = temp_analyzer.compute_scores()

                # S√©lectionner les num√©ros et √©toiles pr√©dits bas√©s sur les scores finaux
                predicted_numbers = sorted(temp_number_scores, key=temp_number_scores.get, reverse=True)[:self.config["propose_size"]]
                predicted_stars = sorted(temp_star_scores, key=temp_star_scores.get, reverse=True)[:self.config["star_size"]]
                
                # Comparer avec le tirage r√©el
                actual_row = self.df.iloc[i]
                actual_numbers = actual_row[self.number_cols].dropna().astype(int).tolist()
                actual_stars = actual_row[self.star_cols].dropna().astype(int).tolist()
                
                # Compter les num√©ros corrects
                correct_numbers = len(set(predicted_numbers) & set(actual_numbers))
                correct_stars = len(set(predicted_stars) & set(actual_stars))
                
                # Calculer les pr√©cisions
                accuracy_numbers = correct_numbers / self.config["propose_size"] if self.config["propose_size"] > 0 else 0
                accuracy_stars = correct_stars / self.config["star_size"] if self.config["star_size"] > 0 else 0
                
                # Enregistrer les r√©sultats
                results['correct_numbers'].append(correct_numbers)
                results['correct_stars'].append(correct_stars)
                results['accuracy_numbers'].append(accuracy_numbers)
                results['accuracy_stars'].append(accuracy_stars)
                
                # Enregistrer la pr√©diction
                results['predictions'].append({
                    'index': i,
                    'date': actual_row['Date'] if 'Date' in actual_row else None,
                    'predicted_numbers': predicted_numbers,
                    'predicted_stars': predicted_stars,
                    'actual_numbers': actual_numbers,
                    'actual_stars': actual_stars,
                    'correct_numbers': correct_numbers,
                    'correct_stars': correct_stars
                })
            
            # Calculer les statistiques globales
            results['avg_correct_numbers'] = sum(results['correct_numbers']) / len(results['correct_numbers']) if results['correct_numbers'] else 0
            results['avg_correct_stars'] = sum(results['correct_stars']) / len(results['correct_stars']) if results['correct_stars'] else 0
            results['avg_accuracy_numbers'] = sum(results['accuracy_numbers']) / len(results['accuracy_numbers']) if results['accuracy_numbers'] else 0
            results['avg_accuracy_stars'] = sum(results['accuracy_stars']) / len(results['accuracy_stars']) if results['accuracy_stars'] else 0
            
            # Distribution des num√©ros corrects
            results['distribution_correct_numbers'] = Counter(results['correct_numbers'])
            results['distribution_correct_stars'] = Counter(results['correct_stars'])
            
            logger.info(f"Backtesting termin√©: {len(results['predictions'])} pr√©dictions √©valu√©es")
            logger.info(f"Nombre moyen de num√©ros corrects: {results['avg_correct_numbers']:.2f}")
            logger.info(f"Nombre moyen d'√©toiles correctes: {results['avg_correct_stars']:.2f}")
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors du backtesting: {str(e)}")
            logger.debug(traceback.format_exc())
            return {}
    
    def plot_backtesting_results(self, results):
        """
        G√©n√®re des visualisations des r√©sultats du backtesting.
        
        Args:
            results: R√©sultats du backtesting
            
        Returns:
            List[str]: Liste des chemins des fichiers de visualisation
        """
        if not results or 'predictions' not in results or not results['predictions']:
            logger.error("R√©sultats de backtesting invalides ou vides")
            return []
        
        logger.info("G√©n√©ration des visualisations de backtesting...")
        
        try:
            # Cr√©er le r√©pertoire de visualisations
            vis_dir = self.output_dir / "visualizations"
            if not vis_dir.exists():
                vis_dir.mkdir(parents=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_files = []
            
            # 1. √âvolution du nombre de num√©ros corrects
            plt.figure(figsize=(12, 6))
            
            indices = [p['index'] for p in results['predictions']]
            correct_numbers = [p['correct_numbers'] for p in results['predictions']]
            correct_stars = [p['correct_stars'] for p in results['predictions']]
            
            plt.plot(indices, correct_numbers, marker='o', label='Num√©ros corrects')
            plt.plot(indices, correct_stars, marker='s', label='√âtoiles correctes')
            
            plt.axhline(y=results['avg_correct_numbers'], color='r', linestyle='--', label=f'Moyenne num√©ros: {results["avg_correct_numbers"]:.2f}')
            plt.axhline(y=results['avg_correct_stars'], color='g', linestyle='--', label=f'Moyenne √©toiles: {results["avg_correct_stars"]:.2f}')
            
            plt.title('√âvolution du nombre de num√©ros et √©toiles corrects')
            plt.xlabel('Indice du tirage')
            plt.ylabel('Nombre corrects')
            plt.grid(alpha=0.3)
            plt.legend()
            
            plot_path = vis_dir / f"backtesting_evolution_{timestamp}.png"
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            
            plot_files.append(str(plot_path))
            
            # 2. Distribution du nombre de num√©ros corrects
            plt.figure(figsize=(10, 6))
            
            dist_numbers = results['distribution_correct_numbers']
            nums = sorted(dist_numbers.keys())
            counts = [dist_numbers[n] for n in nums]
            
            plt.bar(nums, counts, color='royalblue')
            
            plt.title('Distribution du nombre de num√©ros corrects')
            plt.xlabel('Nombre de num√©ros corrects')
            plt.ylabel('Fr√©quence')
            plt.xticks(range(self.config["propose_size"] + 1)) # Adapter les ticks √† la taille de proposition
            plt.grid(axis='y', alpha=0.3)
            
            plot_path = vis_dir / f"backtesting_dist_numbers_{timestamp}.png"
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            
            plot_files.append(str(plot_path))
            
            # 3. Distribution du nombre d'√©toiles correctes
            plt.figure(figsize=(10, 6))
            
            dist_stars = results['distribution_correct_stars']
            stars = sorted(dist_stars.keys())
            counts = [dist_stars[s] for s in stars]
            
            plt.bar(stars, counts, color='gold')
            
            plt.title('Distribution du nombre d\'√©toiles correctes')
            plt.xlabel('Nombre d\'√©toiles correctes')
            plt.ylabel('Fr√©quence')
            plt.xticks(range(self.config["star_size"] + 1)) # Adapter les ticks √† la taille de proposition
            plt.grid(axis='y', alpha=0.3)
            
            plot_path = vis_dir / f"backtesting_dist_stars_{timestamp}.png"
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            
            plot_files.append(str(plot_path))
            
            logger.info(f"{len(plot_files)} visualisations de backtesting g√©n√©r√©es")
            
            return plot_files
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des visualisations de backtesting: {str(e)}")
            logger.debug(traceback.format_exc())
            return []


    def compute_gap_scores(self) -> None:
        """
        Calcule le score d'√©cart pour chaque num√©ro et √©toile.
        L'√©cart est le nombre de tirages depuis la derni√®re apparition.
        Le score est bas√© sur cet √©cart (un √©cart plus grand donne un score plus √©lev√©).
        Les scores sont stock√©s dans self.number_gap_scores et self.star_gap_scores.
        """
        logger.info("Calcul des scores d'√©cart (gap scores)...")
        
        self.number_gap_scores = {}
        self.star_gap_scores = {}
        
        if self.df is None or self.df.empty:
            logger.warning("DataFrame vide, impossible de calculer les scores d'√©cart.")
            return

        total_draws = len(self.df)
        
        # Calcul pour les num√©ros principaux
        for num in range(1, self.config["max_number"] + 1):
            last_occurrence_index = -1
            # Parcourir les tirages en ordre inverse pour trouver la derni√®re occurrence
            # Utiliser .values pour un acc√®s plus rapide aux donn√©es brutes
            found_in_draw = False
            for i in range(total_draws - 1, -1, -1): # Du plus r√©cent au plus ancien
                row_numbers = self.df.iloc[i][self.number_cols].dropna().astype(int).tolist()
                if num in row_numbers:
                    last_occurrence_index = (total_draws - 1) - i # Gap = nombre de tirages depuis
                    found_in_draw = True
                    break
            
            if found_in_draw:
                gap = last_occurrence_index 
            else:
                gap = total_draws # Si jamais apparu, le gap est le nombre total de tirages
                
            # Score simple bas√© sur l'√©cart (un √©cart plus grand = score plus √©lev√©)
            self.number_gap_scores[num] = gap

        # Calcul pour les √©toiles
        for star in range(1, self.config["max_star"] + 1):
            last_occurrence_index = -1
            found_in_draw = False
            for i in range(total_draws - 1, -1, -1):
                row_stars = self.df.iloc[i][self.star_cols].dropna().astype(int).tolist()
                if star in row_stars:
                    last_occurrence_index = (total_draws - 1) - i
                    found_in_draw = True
                    break
            
            if found_in_draw:
                gap = last_occurrence_index
            else:
                gap = total_draws
                
            self.star_gap_scores[star] = gap
            
        logger.info("Calcul des scores d'√©cart termin√©.")



    def compute_scores(self) -> Tuple[Dict[int, float], Dict[int, float]]:
        """
        Calcule les scores finaux pour chaque num√©ro et √©toile en combinant
        les diff√©rentes m√©triques calcul√©es (pr√©dictions ML, scores d'√©cart, fr√©quence, etc.).

        Returns:
            Tuple[Dict[int, float], Dict[int, float]]: Dictionnaires des scores finaux
                                                        pour les num√©ros et les √©toiles.
        """
        logger.info("Calcul des scores finaux...")
        
        number_scores: Dict[int, float] = {}
        star_scores: Dict[int, float] = {}

        # --- Calcul des scores pour les num√©ros --- 
        # Assurer que les dictionnaires de source de score existent
        number_predictions = self.number_predictions if hasattr(self, 'number_predictions') and self.number_predictions else {}
        number_gap_scores = self.number_gap_scores if hasattr(self, 'number_gap_scores') and self.number_gap_scores else {}
        window_counts = self.window_counts if hasattr(self, 'window_counts') and self.window_counts else Counter()

        max_gap_numbers = max(number_gap_scores.values()) if number_gap_scores else 1
        max_freq_numbers = max(window_counts.values()) if window_counts else 1

        for num in range(1, self.config["max_number"] + 1):
            score = 0.0
            weights_sum = 0.0

            # 1. Score bas√© sur les pr√©dictions (ML, fr√©quence, Fibonacci, etc.)
            pred_score = number_predictions.get(num, 0.0)
            score += pred_score * self.config.get("score_weight_prediction", 0.5) 
            weights_sum += self.config.get("score_weight_prediction", 0.5)

            # 2. Score bas√© sur l'√©cart (gap) - Normalis√©
            if max_gap_numbers > 0:
                gap_score = number_gap_scores.get(num, 0) / max_gap_numbers
                score += gap_score * self.config.get("score_weight_gap", 0.3) 
                weights_sum += self.config.get("score_weight_gap", 0.3)

            # 3. Score bas√© sur la fr√©quence r√©cente - Normalis√©
            if max_freq_numbers > 0:
                freq_score = window_counts.get(num, 0) / max_freq_numbers
                score += freq_score * self.config.get("score_weight_frequency", 0.2) 
                weights_sum += self.config.get("score_weight_frequency", 0.2)

            # Normaliser le score final par la somme des poids utilis√©s
            final_score = (score / weights_sum) if weights_sum > 0 else 0.0
            number_scores[num] = final_score

        # --- Calcul des scores pour les √©toiles --- 
        star_predictions = self.star_predictions if hasattr(self, 'star_predictions') and self.star_predictions else {}
        star_gap_scores = self.star_gap_scores if hasattr(self, 'star_gap_scores') and self.star_gap_scores else {}
        star_window_counts = self.star_window_counts if hasattr(self, 'star_window_counts') and self.star_window_counts else Counter()

        max_gap_stars = max(star_gap_scores.values()) if star_gap_scores else 1
        max_freq_stars = max(star_window_counts.values()) if star_window_counts else 1

        for star in range(1, self.config["max_star"] + 1):
            score = 0.0
            weights_sum = 0.0

            # 1. Score bas√© sur les pr√©dictions
            pred_score = star_predictions.get(star, 0.0)
            score += pred_score * self.config.get("score_weight_prediction", 0.5)
            weights_sum += self.config.get("score_weight_prediction", 0.5)

            # 2. Score bas√© sur l'√©cart (gap) - Normalis√©
            if max_gap_stars > 0:
                gap_score = star_gap_scores.get(star, 0) / max_gap_stars
                score += gap_score * self.config.get("score_weight_gap", 0.3)
                weights_sum += self.config.get("score_weight_gap", 0.3)

            # 3. Score bas√© sur la fr√©quence r√©cente - Normalis√©
            if max_freq_stars > 0:
                freq_score = star_window_counts.get(star, 0) / max_freq_stars
                score += freq_score * self.config.get("score_weight_frequency", 0.2)
                weights_sum += self.config.get("score_weight_frequency", 0.2)

            # Normaliser le score final
            final_score = (score / weights_sum) if weights_sum > 0 else 0.0
            star_scores[star] = final_score
            
        # Normaliser les scores finaux pour qu'ils somment √† 1 (si n√©cessaire pour generate_combinations)
        # Ceci est important pour que np.random.choice puisse utiliser ces scores comme probabilit√©s
        total_num_score = sum(number_scores.values())
        if total_num_score > 0:
             number_scores = {num: score / total_num_score for num, score in number_scores.items()}
             
        total_star_score = sum(star_scores.values())
        if total_star_score > 0:
             star_scores = {star: score / total_star_score for star, score in star_scores.items()}

        logger.info("Calcul des scores finaux termin√©.")
        return number_scores, star_scores



    def save_results(self, combinations: List[Tuple[List[int], List[int]]]) -> None:
        """
        Enregistre les combinaisons g√©n√©r√©es dans un fichier texte.

        Args:
            combinations (List[Tuple[List[int], List[int]]]): Liste des combinaisons g√©n√©r√©es.
        """
        logger.info("Enregistrement des combinaisons g√©n√©r√©es...")
        
        try:
            # Cr√©er le r√©pertoire de sortie s'il n'existe pas
            if not self.output_dir.exists():
                self.output_dir.mkdir(parents=True)
            
            # Nom du fichier de r√©sultats
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = self.output_dir / f"combinaisons_euromillions_{timestamp}.txt"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("COMBINAISONS EUROMILLIONS G√âN√âR√âES\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"Date de g√©n√©ration: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Nombre de combinaisons: {len(combinations)}\n\n")
                
                for i, (nums, stars) in enumerate(combinations, 1):
                    num_str = ', '.join(map(str, self._convert_to_int_list(nums)))
                    star_str = ', '.join(map(str, self._convert_to_int_list(stars)))
                    f.write(f"Combinaison {i}: Num√©ros [{num_str}], √âtoiles [{star_str}]\n")
            
            logger.info(f"Combinaisons enregistr√©es dans: {results_file}")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'enregistrement des r√©sultats: {str(e)}")
            logger.debug(traceback.format_exc())



    def _generate_weighted_random_combination(self, number_scores: Dict[int, float], star_scores: Dict[int, float]) -> Tuple[List[int], List[int]]:
        """G√©n√®re une seule combinaison al√©atoire pond√©r√©e par les scores."""
        numbers = []
        stars = []
        
        # V√©rifier si les scores sont valides pour la s√©lection pond√©r√©e
        if not number_scores or not star_scores or sum(number_scores.values()) == 0 or sum(star_scores.values()) == 0:
            logger.warning("Scores non valides pour la g√©n√©ration pond√©r√©e, utilisation d'une s√©lection al√©atoire simple.")
            while len(numbers) < self.config["propose_size"]:
                num = random.randint(1, self.config["max_number"] + 1)
                if num not in numbers:
                    numbers.append(num)
            while len(stars) < self.config["star_size"]:
                star = random.randint(1, self.config["max_star"] + 1)
                if star not in stars:
                    stars.append(star)
            return sorted(numbers), sorted(stars)

        try:
            # S√©lection pond√©r√©e des num√©ros
            # Assurer que les poids sont positifs et somment √† 1
            number_items = list(number_scores.items())
            number_values = [num for num, _ in number_items]
            number_weights = [score for _, score in number_items]
            
            # Normaliser les poids pour np.random.choice
            total_num_weight = sum(number_weights)
            if total_num_weight > 0:
                number_weights = [w / total_num_weight for w in number_weights]
            else: # Fallback si tous les poids sont z√©ro
                number_weights = [1.0 / len(number_values)] * len(number_values)
            
            # Assurer que les poids sont non n√©gatifs et somment √† 1 (tol√©rance pour les flottants)
            if not np.isclose(sum(number_weights), 1.0) or any(w < 0 for w in number_weights):
                logger.warning("Poids des num√©ros invalides apr√®s normalisation, r√©initialisation √† uniforme.")
                number_weights = [1.0 / len(number_values)] * len(number_values)

            while len(numbers) < self.config["propose_size"]:
                num = np.random.choice(number_values, p=number_weights)
                if num not in numbers:
                    numbers.append(num)

            # S√©lection pond√©r√©e des √©toiles
            star_items = list(star_scores.items())
            star_values = [star for star, _ in star_items]
            star_weights = [score for _, score in star_items]
            
            # Normaliser les poids pour np.random.choice
            total_star_weight = sum(star_weights)
            if total_star_weight > 0:
                star_weights = [w / total_star_weight for w in star_weights]
            else: # Fallback si tous les poids sont z√©ro
                star_weights = [1.0 / len(star_values)] * len(star_values)

            # Assurer que les poids sont non n√©gatifs et somment √† 1 (tol√©rance pour les flottants)
            if not np.isclose(sum(star_weights), 1.0) or any(w < 0 for w in star_weights):
                logger.warning("Poids des √©toiles invalides apr√®s normalisation, r√©initialisation √† uniforme.")
                star_weights = [1.0 / len(star_values)] * len(star_values)

            while len(stars) < self.config["star_size"]:
                star = np.random.choice(star_values, p=star_weights)
                if star not in stars:
                    stars.append(star)
            
            return sorted(numbers), sorted(stars)
        except Exception as e:
            logger.error(f"Erreur dans _generate_weighted_random_combination: {e}")
            logger.debug(traceback.format_exc())
            # Fallback vers al√©atoire simple en cas d'erreur
            numbers = random.sample(range(1, self.config["max_number"] + 1), self.config["propose_size"])
            stars = random.sample(range(1, self.config["max_star"] + 1), self.config["star_size"])
            return sorted(numbers), sorted(stars)



    def _is_combination_valid(self, numbers: List[int], stars: List[int]) -> bool:
        """V√©rifie si une combinaison respecte certains filtres heuristiques."""
        # Filtre 1: Somme des num√©ros (utiliser les plages calcul√©es si disponibles)
        if self.sum_stats and self.sum_ranges:
            num_sum = sum(numbers)
            # V√©rifier si la somme tombe dans une plage "raisonnable" (ex: √©viter les extr√™mes)
            # On va consid√©rer les plages qui repr√©sentent un certain pourcentage des tirages historiques
            # Par exemple, rejeter si la somme est dans une plage qui repr√©sente moins de 1% des tirages
            is_sum_in_valid_range = False
            for range_key, stats in self.sum_ranges.items():
                # Extraire les limites de la plage
                start_str, end_str = range_key.split('-')
                start = int(start_str)
                end = int(end_str)
                
                if start <= num_sum <= end and stats['percentage'] >= 1.0: # Minimum 1% des tirages
                    is_sum_in_valid_range = True
                    break
            if not is_sum_in_valid_range:
                # logger.debug(f"Combinaison rejet√©e (somme hors plage valide): {numbers}")
                return False

        # Filtre 2: Parit√© (√©viter les extr√™mes: tout pair ou tout impair, ou des r√©partitions tr√®s rares)
        even_count = sum(1 for num in numbers if num % 2 == 0)
        odd_count = len(numbers) - even_count
        parity_pattern = f"{even_count}E-{odd_count}O"
        
        if self.parity_stats:
            # Rejeter si la configuration de parit√© est tr√®s rare (ex: moins de 1% des tirages)
            if parity_pattern not in self.parity_stats or self.parity_stats[parity_pattern]['percentage'] < 1.0:
                # logger.debug(f"Combinaison rejet√©e (parit√© rare): {numbers}")
                return False
        else: # Fallback si pas de stats de parit√©, rejeter les extr√™mes simples
            if even_count == 0 or even_count == len(numbers):
                # logger.debug(f"Combinaison rejet√©e (parit√© extr√™me - fallback): {numbers}")
                return False


        # Filtre 3: S√©quences (√©viter trop de num√©ros cons√©cutifs)
        sequences = 0
        seq_length = 1
        sorted_numbers = sorted(numbers)
        for i in range(1, len(sorted_numbers)):
            if sorted_numbers[i] == sorted_numbers[i-1] + 1:
                seq_length += 1
            else:
                if seq_length >= 3: # Rejeter si 3 num√©ros cons√©cutifs ou plus
                    sequences += 1
                seq_length = 1
        if seq_length >= 3:
             sequences += 1
        if sequences > 0: # Rejeter si au moins une s√©quence de 3+ num√©ros cons√©cutifs
            # logger.debug(f"Combinaison rejet√©e (s√©quence >= 3): {numbers}")
            return False
            
        # Filtre 4: √âcart entre num√©ros (√©viter les num√©ros trop "serr√©s" ou trop "espac√©s" si cela est rare)
        # Calculer l'√©cart moyen entre les num√©ros tri√©s
        if len(numbers) > 1:
            gaps = [sorted_numbers[i] - sorted_numbers[i-1] for i in range(1, len(sorted_numbers))]
            avg_gap = np.mean(gaps)
            std_gap = np.std(gaps) if len(gaps) > 1 else 0

            # Comparer √† l'√©cart moyen historique (si disponible)
            # Pour l'instant, on n'a pas de stats historiques sur l'√©cart moyen.
            # On peut d√©finir des seuils heuristiques.
            # Par exemple, si l'√©cart moyen est trop petit (num√©ros trop group√©s) ou trop grand (trop espac√©s)
            # Ces seuils sont arbitraires et peuvent √™tre ajust√©s.
            if avg_gap < 5 or avg_gap > 15: # Exemple de seuils
                # logger.debug(f"Combinaison rejet√©e (√©cart moyen des num√©ros hors plage): {numbers} (avg_gap={avg_gap:.2f})")
                pass # D√©sactiv√© pour l'instant, n√©cessite une analyse plus pouss√©e des √©carts historiques

        # Filtre 5: Distance par rapport aux tirages pr√©c√©dents (√©viter r√©p√©tition exacte r√©cente)
        # Cela est d√©j√† g√©r√© par `seen_combos` dans `generate_combinations` pour l'unicit√©
        # Si on veut √©viter les combinaisons "trop similaires" aux r√©centes, il faudrait une m√©trique de similarit√©
        # et un seuil, ce qui est plus complexe. Pour l'instant, on se concentre sur l'unicit√© exacte.

        return True # La combinaison a pass√© tous les filtres

    def _run_monte_carlo_simulation(self, number_scores: Dict[int, float], star_scores: Dict[int, float]) -> List[Tuple[List[int], List[int]]]:
        """Ex√©cute la simulation Monte Carlo pour g√©n√©rer des combinaisons filtr√©es."""
        valid_combinations = []
        seen_combos = set()
        num_simulations = self.config.get("monte_carlo_simulations", 10000) 
        target_combinations_to_find = self.config["combinations_to_generate"] * 2 # Chercher plus pour avoir du choix
        max_attempts = num_simulations * 5 # Limite pour √©viter boucle infinie

        logger.info(f"Lancement de {num_simulations} simulations Monte Carlo...")

        attempts = 0
        while len(valid_combinations) < target_combinations_to_find and attempts < max_attempts: 
            attempts += 1
            # G√©n√©rer une combinaison candidate pond√©r√©e
            candidate_numbers, candidate_stars = self._generate_weighted_random_combination(number_scores, star_scores)
            
            # V√©rifier l'unicit√©
            combo_tuple = (tuple(sorted(candidate_numbers)), tuple(sorted(candidate_stars)))
            if combo_tuple in seen_combos:
                continue
                
            # Appliquer les filtres heuristiques
            if self._is_combination_valid(candidate_numbers, candidate_stars):
                valid_combinations.append((candidate_numbers, candidate_stars))
                seen_combos.add(combo_tuple)
                if attempts % (num_simulations // 10 if num_simulations >= 10 else 1) == 0:
                     logger.debug(f"Monte Carlo: {len(valid_combinations)} combinaisons valides trouv√©es apr√®s {attempts} tentatives.")

        logger.info(f"Simulation Monte Carlo termin√©e: {len(valid_combinations)} combinaisons valides trouv√©es apr√®s {attempts} tentatives.")
        # Retourner seulement le nombre requis, potentiellement moins si pas assez trouv√©es
        return valid_combinations[:self.config["combinations_to_generate"]] # Retourne seulement le nombre final d√©sir√©


    def _calculate_combination_rank(self, generated_nums: List[int], generated_stars: List[int], actual_nums: List[int], actual_stars: List[int]) -> Optional[str]:
        """Calcule le rang d'une combinaison g√©n√©r√©e par rapport √† un tirage r√©el."""
        matched_nums = len(set(generated_nums) & set(actual_nums))
        matched_stars = len(set(generated_stars) & set(actual_stars))

        # D√©finir les rangs (simplifi√©, peut √™tre ajust√© selon les r√®gles officielles)
        if matched_nums == 5 and matched_stars == 2: return "Rang 1 (5+2)"
        if matched_nums == 5 and matched_stars == 1: return "Rang 2 (5+1)"
        if matched_nums == 5 and matched_stars == 0: return "Rang 3 (5+0)"
        if matched_nums == 4 and matched_stars == 2: return "Rang 4 (4+2)"
        if matched_nums == 4 and matched_stars == 1: return "Rang 5 (4+1)"
        if matched_nums == 3 and matched_stars == 2: return "Rang 6 (3+2)"
        if matched_nums == 4 and matched_stars == 0: return "Rang 7 (4+0)"
        if matched_nums == 2 and matched_stars == 2: return "Rang 8 (2+2)"
        if matched_nums == 3 and matched_stars == 1: return "Rang 9 (3+1)"
        if matched_nums == 3 and matched_stars == 0: return "Rang 10 (3+0)"
        if matched_nums == 1 and matched_stars == 2: return "Rang 11 (1+2)"
        if matched_nums == 2 and matched_stars == 1: return "Rang 12 (2+1)"
        if matched_nums == 2 and matched_stars == 0: return "Rang 13 (2+0)"
        
        return None # Aucun gain

    def track_performance(self):
        """Analyse la performance des combinaisons g√©n√©r√©es historiquement."""
        logger.info("Analyse de la performance historique des combinaisons g√©n√©r√©es...")
        if not self.generated_combinations_history or self.df is None or 'Date' not in self.df.columns:
            logger.warning("Historique des combinaisons ou donn√©es de tirage manquantes pour l'analyse de performance.")
            return

        # S'assurer que la colonne Date est bien au format datetime
        if not pd.api.types.is_datetime64_any_dtype(self.df['Date']):
            try:
                self.df['Date'] = pd.to_datetime(self.df['Date'])
            except Exception as e:
                logger.error(f"Impossible de convertir la colonne 'Date' en datetime pour le suivi de performance: {e}")
                return
        
        # Trier le DataFrame par date au cas o√π
        df_sorted = self.df.sort_values('Date').reset_index(drop=True) # drop=True pour ne pas ajouter l'ancien index
        
        new_performance_entries = []
        processed_dates = {entry['generation_date'] for entry in self.performance_history} # Pour √©viter doublons

        for generation_date_str, combinations in self.generated_combinations_history:
            if generation_date_str in processed_dates:
                continue # D√©j√† trait√©

            # Trouver l'index du tirage suivant la date de g√©n√©ration
            next_draw_index = -1
            try:
                generation_date = datetime.strptime(generation_date_str, '%Y-%m-%d')
                # Chercher le premier tirage APR√àS la date de g√©n√©ration
                # Utiliser idxmax pour trouver le premier index o√π la condition est vraie
                after_generation_draws = df_sorted[df_sorted['Date'] > generation_date]
                if not after_generation_draws.empty:
                    next_draw_index = after_generation_draws.index[0]
            except ValueError:
                 logger.warning(f"Format de date invalide dans l'historique: {generation_date_str}")
                 continue

            if next_draw_index == -1:
                logger.debug(f"Aucun tirage trouv√© apr√®s la date {generation_date_str} pour √©valuer la performance.")
                continue

            # Obtenir le tirage r√©el
            actual_draw = df_sorted.iloc[next_draw_index]
            actual_nums = actual_draw[self.number_cols].dropna().astype(int).tolist()
            actual_stars = actual_draw[self.star_cols].dropna().astype(int).tolist()
            actual_draw_date_str = actual_draw['Date'].strftime('%Y-%m-%d')

            # √âvaluer chaque combinaison g√©n√©r√©e pour cette date
            results = {'total_combinations': len(combinations), 'wins': {}}
            best_rank = None
            best_rank_num = 99 # Pour trier (plus petit = meilleur)

            for nums, stars in combinations:
                rank = self._calculate_combination_rank(nums, stars, actual_nums, actual_stars)
                if rank:
                    # Extraire le num√©ro du rang (ex: "Rang 1 (5+2)" -> 1)
                    try:
                        rank_num = int(rank.split(' ')[1].replace('(', ''))
                    except (IndexError, ValueError):
                        rank_num = 99 # Valeur par default si le format n'est pas celui attendu
                    
                    results['wins'][rank] = results['wins'].get(rank, 0) + 1
                    if rank_num < best_rank_num:
                        best_rank_num = rank_num
                        best_rank = rank
            
            performance_entry = {
                'generation_date': generation_date_str,
                'evaluated_draw_date': actual_draw_date_str,
                'best_rank_achieved': best_rank,
                'win_distribution': results['wins']
            }
            new_performance_entries.append(performance_entry)
            processed_dates.add(generation_date_str)

        # Ajouter les nouvelles entr√©es √† l'historique et trier
        self.performance_history.extend(new_performance_entries)
        self.performance_history.sort(key=lambda x: x['generation_date'])
        
        # Limiter la taille de l'historique de performance
        max_perf_history = self.config.get("max_performance_history", 100)
        if len(self.performance_history) > max_perf_history:
             self.performance_history = self.performance_history[-max_perf_history:]

        logger.info(f"Analyse de performance termin√©e. {len(new_performance_entries)} nouvelles √©valuations ajout√©es.")

    def load_history(self, history_file="analysis_history.joblib"):
        """Charge l'historique des combinaisons et performances depuis un fichier."""
        history_path = self.output_dir / history_file
        if history_path.exists():
            try:
                data = joblib.load(history_path)
                self.generated_combinations_history = data.get('combinations', [])
                self.performance_history = data.get('performance', [])
                logger.info(f"Historique charg√© depuis {history_path}")
            except Exception as e:
                logger.error(f"Erreur lors du chargement de l'historique depuis {history_path}: {e}")
        else:
            logger.info("Aucun fichier d'historique trouv√©, d√©marrage avec un historique vide.")

    def save_history(self, history_file="analysis_history.joblib"):
        """Sauvegarde l'historique des combinaisons et performances dans un fichier."""
        history_path = self.output_dir / history_file
        try:
            data = {
                'combinations': self.generated_combinations_history,
                'performance': self.performance_history
            }
            joblib.dump(data, history_path)
            logger.info(f"Historique sauvegard√© dans {history_path}")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde de l'historique dans {history_path}: {e}")



    def _generate_wheeling_combinations(self, numbers_to_wheel: List[int], stars_to_wheel: List[int], num_guarantee: int = 3, star_guarantee: int = 1) -> List[Tuple[List[int], List[int]]]:
        """
        G√©n√®re des combinaisons en utilisant un syst√®me r√©ducteur simple (garantie basique).
        Ceci est une impl√©mentation basique, des syst√®mes plus complexes existent.

        Args:
            numbers_to_wheel (List[int]): Liste des num√©ros principaux √† couvrir.
            stars_to_wheel (List[int]): Liste des √©toiles √† couvrir.
            num_guarantee (int): Garantie minimale pour les num√©ros (ex: 3 si 4).
            star_guarantee (int): Garantie minimale pour les √©toiles (ex: 1 si 2).

        Returns:
            Liste de combinaisons g√©n√©r√©es par le syst√®me r√©ducteur.
        """
        logger.info(f"G√©n√©ration de combinaisons via syst√®me r√©ducteur (Num√©ros: {len(numbers_to_wheel)} pour {self.config['propose_size']}, √âtoiles: {len(stars_to_wheel)} pour {self.config['star_size']})...")
        wheeled_combinations = []
        
        # V√©rifier si les listes sont assez grandes
        if len(numbers_to_wheel) < self.config['propose_size'] or len(stars_to_wheel) < self.config['star_size']:
            logger.warning("Pas assez de num√©ros/√©toiles fournis pour le syst√®me r√©ducteur. Fallback √† la s√©lection al√©atoire parmi les fournis.")
            # Fallback: g√©n√©rer une combinaison al√©atoire √† partir des num√©ros fournis
            # S'assurer qu'on ne demande pas plus d'√©l√©ments qu'il n'y en a
            nums_sample_size = min(self.config['propose_size'], len(numbers_to_wheel))
            stars_sample_size = min(self.config['star_size'], len(stars_to_wheel))

            if nums_sample_size > 0:
                 nums = random.sample(numbers_to_wheel, nums_sample_size)
            else:
                 nums = [] # Ou g√©n√©rer totalement al√©atoire si numbers_to_wheel est vide
                 
            if stars_sample_size > 0:
                 stars = random.sample(stars_to_wheel, stars_sample_size)
            else:
                 stars = [] # Ou g√©n√©rer totalement al√©atoire si stars_to_wheel est vide
            
            # Compl√©ter avec des num√©ros/√©toiles al√©atoires si les listes initiales √©taient trop petites
            while len(nums) < self.config['propose_size']:
                new_num = random.randint(1, self.config['max_number'])
                if new_num not in nums:
                    nums.append(new_num)
            while len(stars) < self.config['star_size']:
                new_star = random.randint(1, self.config['max_star'])
                if new_star not in stars:
                    stars.append(new_star)

            return [(sorted(nums), sorted(stars))]

        try:
            # G√©n√©rer toutes les combinaisons possibles de la taille requise √† partir des num√©ros fournis
            # Ceci n'est PAS un vrai syst√®me r√©ducteur optimis√©, mais une simple g√©n√©ration de toutes les combinaisons
            # Un vrai syst√®me r√©ducteur s√©lectionnerait un sous-ensemble minimal pour garantir la couverture
            num_combos = list(iter_combinations(numbers_to_wheel, self.config['propose_size']))
            star_combos = list(iter_combinations(stars_to_wheel, self.config['star_size']))

            # Limiter le nombre de combinaisons g√©n√©r√©es pour √©viter une explosion combinatoire
            max_wheeled = self.config.get("max_wheeling_combinations", 50)
            count = 0
            # Combiner les num√©ros et les √©toiles (ici, on prend juste les premi√®res combinaisons)
            # Une approche plus sophistiqu√©e est n√©cessaire pour une vraie garantie
            for n_combo in num_combos:
                for s_combo in star_combos:
                    if count < max_wheeled:
                        wheeled_combinations.append((sorted(list(n_combo)), sorted(list(s_combo))))
                        count += 1
                    else:
                        break
                if count >= max_wheeled:
                    break
            
            logger.info(f"{len(wheeled_combinations)} combinaisons g√©n√©r√©es par la m√©thode de couverture (limit√© √† {max_wheeled}).")
            return wheeled_combinations

        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des combinaisons r√©ductrices: {e}")
            logger.debug(traceback.format_exc())
            return []

